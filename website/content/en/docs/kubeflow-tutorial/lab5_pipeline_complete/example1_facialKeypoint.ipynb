{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ebd88b-d000-45cb-9e34-7d149c348884",
   "metadata": {},
   "source": [
    "# Lab 5: Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5d621-1e3c-4255-82e5-a5dfd6a7c324",
   "metadata": {},
   "source": [
    "### Install Kubeflow Pipeline Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38374f8b-4ffa-4b12-a3d0-440fe0f93c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277f10a-fa1f-4ace-a3e4-5caf08f3f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the kfp sdk\n",
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16af72-cdc0-4439-a454-9ee3326ef924",
   "metadata": {},
   "source": [
    "## Example 1: Facial Keypoint Detection\n",
    "\n",
    "In this example, we would build pipeline components from ***docker images***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff00c16-7c0e-45fd-9b5b-145914bd82a1",
   "metadata": {},
   "source": [
    "### About this Model\n",
    "\n",
    "This model comes from Kaggle Competition. The objective of this task is to predict keypoint positions on face images, which can be used as a building block in several applications, such as analysing facial expressions and biometrics recognition.\n",
    "\n",
    "There are two main tasks: train and evaluation. Each would be build as a pipeline component later. \n",
    "\n",
    "Please download datasets at https://www.kaggle.com/competitions/facial-keypoints-detection and put them at the path: `train/my_data`, and you can also find more details about this model itself. Specifically, You need to download `test.zip` and `train.zip` files and put them under `train/my_data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44beec68-b95f-4140-ba4d-e88ac6943b9c",
   "metadata": {},
   "source": [
    "### Design Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5735a48-fc53-43fc-88d8-765ac6070065",
   "metadata": {},
   "source": [
    "When Kubeflow Pipelines executes a component, a container image is started in a Kubernetes Pod and your component’s inputs are passed in as command-line arguments.\n",
    "\n",
    "Therefore, while designing pipeline components, we need to consider following issues:\n",
    "* Which inputs can be passed to our component by value? And which inputs, which cannot be directly passed as a command-line argument, should be passed to your component by a reference to the input’s path?\n",
    "* To return an output from our component, the output’s data must be stored as a file. We need to let Kubeflow Pipelines know what outputs our component produces, so that when our pipeline runs, Kubeflow Pipelines can pass the paths that we use to store our component’s outputs as inputs to our component.\n",
    "* Since your inputs and output paths are passed in as command-line arguments, your component’s code must be able to read inputs from the command line. \n",
    "\n",
    "And in this example, specifically, a component specification should define:\n",
    "* The component’s inputs and outputs\n",
    "* The container image that your component’s code runs in, the command to use to run your component’s code, and the command-line arguments to pass to your component’s code\n",
    "* The component’s metadata, such as the name and description\n",
    "\n",
    "Note that here as we are going to build each component from docker images, ***<span style=\"color:blue\">you do not need to run</span>*** following code blocks for train and evaluation in this notebook. We mainly guide you through the flow of each component design in this design section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ee9f8-4b9c-4dba-a608-781f532bd219",
   "metadata": {},
   "source": [
    "#### Design Train Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b1d22-f708-4582-a668-6567632238a2",
   "metadata": {},
   "source": [
    "We first design the component for training. Codes can be found in [train/train.py](./train/train.py). \n",
    "\n",
    "Train component takes three inputs: `trial`, `epoch`, and `patience`, and would export the trained model as output which would later be used as input of model evaluation.\n",
    "\n",
    "Most codes follow the original workflow of the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e456c0-d985-4f79-9eed-3c9e992af6d7",
   "metadata": {},
   "source": [
    "**Import packages**\n",
    "```python\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle           \n",
    "import matplotlib.pyplot as plt             \n",
    "import tensorflow as tf                \n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import autokeras as ak\n",
    "```\n",
    "Among above packages, `argparse` is specifically useful for our pipeline component design. This package would be used later to parse command-line arguments into component inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcc91c-98be-40fb-9dc8-3adda2dec9d7",
   "metadata": {},
   "source": [
    "**Declare input parameters**\n",
    "\n",
    "Remember that when Kubeflow Pipelines executes a component, this component’s inputs are passed in as command-line arguments. So here we need to define and parse the command-line arguments, using `argparse`.\n",
    "```python\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--trial', type=int)\n",
    "parser.add_argument('--epoch', type=int)\n",
    "parser.add_argument('--patience', type=int)\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "trials = args['trial']\n",
    "epochs = args['epoch']\n",
    "patience = args['patience']\n",
    "```\n",
    "In this example, the Train component takes three inputs as parameters: `trial`, `epoch`, and `patience`. You would need to specify these three inputs before running this pipeline. We would discuss this more in details later in running pipeline section.\n",
    "\n",
    "Some metadata of this model is declared and defined then, following the model design itself.\n",
    "```python\n",
    "project=\"Facial-keypoints\"\n",
    "run_id= \"1.8\"\n",
    "resume_run = True\n",
    "\n",
    "MAX_TRIALS=trials\n",
    "EPOCHS=epochs\n",
    "PATIENCE=patience\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1291ce-7dba-4173-8cd5-2c8446dec957",
   "metadata": {},
   "source": [
    "**Extract data**\n",
    "\n",
    "The model then extracts data and saves the data to attached *extenal PVC* at location `/data`. We would later need to specifyc this PVC and let Kubeflow pipelines know what outputs this Train component would produce and where to store later in component specification.\n",
    "\n",
    "Training dataset and test dataset, in this example, should be stored in [train/my_data](./train/my_data). **Remember to change this part if you change the path of datasets storage.**\n",
    "```python\n",
    "base_dir='./train/my_data/'\n",
    "train_dir_zip=base_dir+'training.zip'\n",
    "test_dir_zip=base_dir+'test.zip'\n",
    "\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(train_dir_zip,'r') as zipObj:\n",
    "    zipObj.extractall('/data')\n",
    "    print(\"Train Archive unzipped\")\n",
    "with ZipFile(test_dir_zip,'r') as zipObj:\n",
    "    zipObj.extractall('/data')\n",
    "    print(\"Test Archive unzipped\")\n",
    "```\n",
    "Note that the overall flow of this data extraction part follows from the original model. The only things we need to change for pipeline component design is the path, i.e. location, for data storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc74ebc-78ac-490b-845e-42cafa16ce3b",
   "metadata": {},
   "source": [
    "**Process data**\n",
    "\n",
    "This part, along with following data training part, follow from the model itself. No more changes needed for pipeline component design for this part.\n",
    "```python\n",
    "train_dir='/data/training.csv'\n",
    "test_dir='/data/test.csv'\n",
    "train=pd.read_csv(train_dir)\n",
    "test=pd.read_csv(test_dir)\n",
    "\n",
    "train=train.dropna()\n",
    "train=train.reset_index(drop=True)\n",
    "\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "\n",
    "for img in train['Image']:\n",
    "    X_train.append(np.asarray(img.split(),dtype=float).reshape(96,96,1))\n",
    "X_train=np.reshape(X_train,(-1,96,96,1))\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "    \n",
    "for i in range(len((train))): \n",
    "    Y_train.append(np.asarray(train.iloc[i][0:30].to_numpy()))\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835506f-5493-4453-999c-8186f205565e",
   "metadata": {},
   "source": [
    "**Train data**\n",
    "```python\n",
    "reg = ak.ImageRegressor(max_trials=MAX_TRIALS)\n",
    "reg.fit(X_train, Y_train, validation_split=0.15, epochs=EPOCHS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb709c15-5d7d-4203-9856-46532f282e3b",
   "metadata": {},
   "source": [
    "**Export trained model**\n",
    "\n",
    "Finally, we need to export our trained model to our externally attached PVC, so that our Evaluate component can then take this trained model as input.\n",
    "```python\n",
    "my_model = reg.export_model()\n",
    "my_model.save('/data/model_autokeras', save_format=\"tf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5f2d6-7ebc-4668-b826-4a9cac104c65",
   "metadata": {},
   "source": [
    "#### Design Evaluation Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553c0e1-b634-427f-b77c-5281aa76a9d1",
   "metadata": {},
   "source": [
    "We then design our Evaluation component. Codes can be found in [evaluation/eval.py](./evaluate/eval.py). \n",
    "\n",
    "This component takes the trained model as input. Some of the results, the submission file, would be directly printed out in log. The complete results, which is a pretty big file, would be saved as a `.csv` in PVC.\n",
    "\n",
    "The overall logic follows from original model design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc033b-e37c-49d1-976f-42cdee55442b",
   "metadata": {},
   "source": [
    "**Import packages**\n",
    "```python\n",
    "from tensorflow.keras.models import load_model\n",
    "import autokeras as ak\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbea5a-02a7-48ca-a410-fcb791bd483e",
   "metadata": {},
   "source": [
    "**Load and view trained model**\n",
    "\n",
    "First, we need to load our trained model, the output of our Train component.\n",
    "```python\n",
    "loaded_model = load_model(\"/data/model_autokeras\", custom_objects=ak.CUSTOM_OBJECTS)\n",
    "```\n",
    "You may print the trained model summary, and you can see these printed contents in `main-logs` in `Output artifacts` on Kubeflow UI after the pipeline finishes running. More details on this would be discussed later in running pipeline section.\n",
    "```python\n",
    "### Pint model summary\n",
    "print(loaded_model.summary())\n",
    "\n",
    "test_dir='/data/test.csv'\n",
    "test=pd.read_csv(test_dir)\n",
    "\n",
    "X_test=[]\n",
    "for img in test['Image']:\n",
    "    X_test.append(np.asarray(img.split(),dtype=float).reshape(96,96,1))\n",
    "X_test=np.reshape(X_test,(-1,96,96,1))\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb33a8c-2637-44a9-9302-1fda306dbca5",
   "metadata": {},
   "source": [
    "**Predict**\n",
    "\n",
    "```python\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced45744-d65d-42f2-88b3-8973feec31c2",
   "metadata": {},
   "source": [
    "**Create submission file**\n",
    "\n",
    "As the submission file is pretty big, we store it under `/data` in our PVC container, the same place where we extract our training and testing data into. For you to have a quick look, we also directly print part of it, which would be displayed in `main-logs` after pipeline finishes running.\n",
    "```python\n",
    "y_pred= y_pred.reshape(-1,)\n",
    "submission = pd.DataFrame({'Location': y_pred})\n",
    "submission.to_csv('/data/submission.csv', index=True , index_label='RowId')\n",
    "\n",
    "res = pd.read_csv('/data/submission.csv')\n",
    "print()\n",
    "print('***********************************************')\n",
    "print(res)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cc259-0450-4b75-8b97-272d8b540c70",
   "metadata": {},
   "source": [
    "### Containernize Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8d4ad-1112-4a45-bb86-789c81b7ad8a",
   "metadata": {},
   "source": [
    "Now, we have gone through and understood the logic of pipeline component design. We then start to containernize our pipeline components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc8a0a-5434-49e5-a63f-80bef8a683c7",
   "metadata": {},
   "source": [
    "#### Write Dockerfile\n",
    "We use Docker to build images. Basically, Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. \n",
    "\n",
    "Instructions and details of how to write a Dockerfile can be found on [Docker's official docs](https://docs.docker.com/engine/reference/builder/).\n",
    "\n",
    "In this example, we provide you with following Dockerfile for Train component and Evaluate component.\n",
    "```dockerfile\n",
    "FROM \"ubuntu\"\n",
    "RUN apt-get update && yes | apt-get upgrade\n",
    "RUN mkdir -p /tensorflow/models\n",
    "RUN apt-get install -y git python3-pip\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN pip3 install tensorflow\n",
    "RUN pip3 install jupyter\n",
    "RUN pip3 install matplotlib\n",
    "RUN pip3 install kfp==1.1.2\n",
    "RUN pip install opencv-python-headless\n",
    "RUN pip3 install pandas keras \n",
    "RUN pip3 install sklearn\n",
    "RUN pip3 install autokeras\n",
    "COPY . /\n",
    "```\n",
    "Codes can be found in both `train/Dockerfile` and `evaluate/Dockerfile`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596da44-7937-4903-8e8d-c9831563eac4",
   "metadata": {},
   "source": [
    "#### Build Docker Images\n",
    "Build docker images for Train component and Evaluate component using `docker run` command.\n",
    "\n",
    "More details about `docker run` commands can be found on [here](https://docs.docker.com/engine/reference/commandline/run/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18167c-f84b-4445-97ea-ebf53d9957a3",
   "metadata": {},
   "source": [
    "***OPTION 1***\n",
    "\n",
    "We have already prepared you built image. If you did not personalize the codes or change the paths of files, feel free to directly use them and skip this part. The location of the image is already in pipeline generation codes. \n",
    "\n",
    "You may also pull the image to your local if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ce649-1bb1-4ea6-a949-a48bbc8efefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull projects.registry.vmware.com/kubeflow/lab_pipeline@sha256:3418cabc178b04a24e0f2b767ccaf4cc0e3fad68c3a6f407b4508ace433b5d83"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c585c35-62e6-4e3a-9231-a5c0b05c2d84",
   "metadata": {},
   "source": [
    "***OPTION 2***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d3fb1-f803-4415-bac9-7bb321bf0451",
   "metadata": {},
   "source": [
    "Or, you can also build Docker images on your own locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8cef79-6d33-4e2c-a8cc-f9989f72bd72",
   "metadata": {},
   "source": [
    "**Install Docker**\n",
    "\n",
    "Make sure Docker is [installed](https://docs.docker.com/engine/install/) in your environment. And login after install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302fe01-b86f-4786-943a-75af646ee5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a86da-e4be-41d0-b6e0-1f257badd7a3",
   "metadata": {},
   "source": [
    "**Build images**\n",
    "\n",
    "Use `docker build` command to build Docker images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bbb7e-bb37-4b50-80fa-2202efd72b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --platform linux/amd64 -f <dockerfile_path> -t <docker_username>/<docker_imagename>:<tag> ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095eef93-afe5-4818-bb49-e8afffb372d0",
   "metadata": {},
   "source": [
    "If you only have one Dockerfile where you run above command, you may not need to specify `-f <dockerfile_path>`. \n",
    "\n",
    "Feel free to directly run following command to build docker image in this notebook, if you did not change paths of files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba51eb7-ecd3-4a73-acb8-5fde570e9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --platform linux/amd64 -t docker_images:facial ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5814e-ab49-4be3-bb22-61b3cc73f3d0",
   "metadata": {},
   "source": [
    "Or, you can also build images for train and evaluate separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b42a4-171a-40ee-be35-04ec1e6563ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597756b4-70d5-482d-bf33-3c11a1039201",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --platform linux/amd64 -t docker_images:facial_train ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65a45e-0f3a-4e0d-93ab-fe63808e20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3d1c7-f227-4f0d-a2b2-d42c37cf3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --platform linux/amd64 -t docker_images:facial_eval ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ebdda-4d14-4003-82e6-4a9faa73a4bc",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcf33b-d932-4e71-9343-f6a090f45db6",
   "metadata": {},
   "source": [
    "So far we have finished designing pipeline components and containernizing them. It is now time for our pipeline generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea035dc-28b3-4509-a87e-d469be13b7d8",
   "metadata": {},
   "source": [
    "#### Create Component Specifications\n",
    "As discussed at the beginning introduction part, we need to first define component specifications which include\n",
    "* The component’s inputs and outputs\n",
    "* The container image that your component’s code runs in, the command to use to run your component’s code, and the command-line arguments to pass to your component’s code\n",
    "* The component’s metadata, such as the name and description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8e7ff-af7a-4ad0-b256-55fbb83bab4d",
   "metadata": {},
   "source": [
    "**Import Kubeflow Pipeline packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af1975-7afc-4af3-bc95-c62e1bad4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9e6fd-bb3b-430d-8936-c4db6f174019",
   "metadata": {},
   "source": [
    "**Login to Docker if necessary**\n",
    "\n",
    "If you want to use your own docker images stored locally, you may need to first login to Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cf127-ddc6-4022-8c63-8607969c04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login # is not needed if you use our images, or store your image somewhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbfae3-2092-423e-bc50-ce5e92279102",
   "metadata": {},
   "source": [
    "**Train Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4ccaa-8f58-462d-ad5b-f9933cd610b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train component takes three inputs: trial, epoch, and patience\n",
    "# return a ContainerOp instance, representing Train step in pipeline\n",
    "\n",
    "def Train(trial, epoch, patience):\n",
    "    vop = dsl.VolumeOp(name=\"pvc\",\n",
    "                       resource_name=\"pvc\", size='1Gi', \n",
    "                       modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Train', \n",
    "        image = 'projects.registry.vmware.com/kubeflow/lab_pipeline@sha256:3418cabc178b04a24e0f2b767ccaf4cc0e3fad68c3a6f407b4508ace433b5d83',\n",
    "        command = ['python3', '/train/train.py'],\n",
    "        arguments=[\n",
    "            '--trial', trial,\n",
    "            '--epoch', epoch,\n",
    "            '--patience', patience\n",
    "        ],\n",
    "        pvolumes={\n",
    "            '/data': vop.volume\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c96d6a-0d2c-45c6-a71d-0233bb5d61ce",
   "metadata": {},
   "source": [
    "First, we need to create and specify the persistent volume (PVC) for data storage, creating a `VolumeOP` instance.\n",
    "```python\n",
    "vop = dsl.VolumeOp(name=\"pvc\",\n",
    "                       resource_name=\"pvc\", size='1Gi', \n",
    "                       modes=dsl.VOLUME_MODE_RWO)\n",
    "```\n",
    "We then create a `ContainerOp` instance, which would be understood and used as \"a step\" in our pipeline, and return this \"step\".\n",
    "```python\n",
    "return dsl.ContainerOp(\n",
    "        name = 'Train', \n",
    "        image = 'projects.registry.vmware.com/kubeflow/lab_pipeline@sha256:3418cabc178b04a24e0f2b767ccaf4cc0e3fad68c3a6f407b4508ace433b5d83', \n",
    "        command = ['python3', '/train/train.py'],\n",
    "        arguments=[\n",
    "            '--trial', trial,\n",
    "            '--epoch', epoch,\n",
    "            '--patience', patience\n",
    "        ],\n",
    "        pvolumes={\n",
    "            '/data': vop.volume\n",
    "        }\n",
    "    )\n",
    "```\n",
    "We need to specify the inputs (`trial`, `epoch`, and `patience`) in `arguments`, container image in `image`, and volume for data storage in `pvolumes`. Note that here in `image`, we provide you with our built images, containing both `train` folder and `evaluate` folder, stored on our `projects.registry` repo. If you want to use your own image, please remember to change this value.\n",
    "\n",
    "We also need to specify `command`. In this provided case, as we containernize the image at root directory, in command we need `python3 /train/train.py`. (If you containernize Train component and Evaluate component one by one in each own folder, you may need to change this value to `python3 train.py`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff2293-e979-4ec9-b4ed-edf96527802f",
   "metadata": {},
   "source": [
    "**Evaluate Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e004e6a-60f8-4d9b-a890-9054be39bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate component takes Train ContainerOp as input, and access Train ContainerOp's pvolumes to get the trained model stored in it\n",
    "# return a ContainerOp instance representing Evaluate step in pipeline\n",
    "\n",
    "def Evaluate(comp1):\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Evaluate',\n",
    "        image = 'projects.registry.vmware.com/kubeflow/lab_pipeline@sha256:3418cabc178b04a24e0f2b767ccaf4cc0e3fad68c3a6f407b4508ace433b5d83',\n",
    "        pvolumes={\n",
    "            '/data': comp1.pvolumes['/data']\n",
    "        },\n",
    "        command = ['python3', '/eval/eval.py']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825806a-db85-4192-a3ef-0f7c8d710303",
   "metadata": {},
   "source": [
    "Again, we need to create a `ContainerOp` instance and return it, to be used as a step in our pipeline.\n",
    "\n",
    "Here, we provide container image in `image`, and command to run the python file for evaluation in `command`. Similary, remember to change these two values if you want to use your own docker images or if you containernize the component under different directory.\n",
    "\n",
    "For Evaluate component, it does not need explicit argument by value. Instead, it takes the trained model as input. This trained model is generated by Train component, and cannot be passed directly by value, so we need to \"pass\" it by reference. The way we do this here is to store the trained model in `/data`, our attached externally PVC, so that as long as we specify this PVC here in `pvolumes`, the Evaluate component would be able to access our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1d468-3294-4a0e-b5fa-fcdcb85d0b65",
   "metadata": {},
   "source": [
    "#### Generate Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f5c4d-d47f-46f5-9af2-225bf69434e4",
   "metadata": {},
   "source": [
    "We are now ready to define out `pipeline` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0470db-d56e-4ced-8fae-f28431ba84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name = 'facial keypoints detection pipeline',\n",
    "    description = 'pipeline to detect facial keypoints')\n",
    "def generate_pipeline(trial, epoch, patience):\n",
    "    comp1 = Train(trial, epoch, patience)\n",
    "    comp2 = Evaluate(comp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedde998-f8c9-4058-b3b2-8ae3a35c7783",
   "metadata": {},
   "source": [
    "Run above function to build our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f34825-f534-4fa2-9e1e-7cc26869a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  import kfp.compiler as compiler\n",
    "  compiler.Compiler().compile(generate_pipeline, 'face_pipeline' + '.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735002e1-6094-418c-807a-b6fd5ead2e63",
   "metadata": {},
   "source": [
    "You should now be able to see a file called `face_pipeline.yaml` in current directory, same as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffca40-1292-4f7c-b985-24da42da2266",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b205c-48e5-4b01-8575-55f4e9e6fb2c",
   "metadata": {},
   "source": [
    "In our last cell, we compile our pipeline as a YAML file. \n",
    "\n",
    "Note that for testing purpose, we also provide you with two already-compiled pipeline YAML files, `face_pipeline_test.yaml`. Feel free to directly download and use them.\n",
    "\n",
    "To run this pipeline, go to Kubeflow UI. Navigate to Pipelines Page. Upload this pipeline by choosing \"upload a file\" option. Choose the YAML file we created just now.\n",
    "\n",
    "![face1](./img/face1.png)\n",
    "\n",
    "![face2](./img/face2.png)\n",
    "\n",
    "![face3](./img/face3.png)\n",
    "\n",
    "After the pipeline uploading process finishes, you should be able to see the pipeline graph. Create a experiment for this pipeline, and then create a run.\n",
    "\n",
    "![face4](./img/face4.png)\n",
    "\n",
    "You need to enter values for the three required inputs for Train: trial, epoch, and patience.\n",
    "\n",
    "![face5](./img/face5.png)\n",
    "\n",
    "The pipeline would start to run then. You would be able to see the running process in Runs Page on Kubeflow UI.\n",
    "\n",
    "![face6](./img/face6.png)\n",
    "\n",
    "The pipeline running may take some time, especially when you input a large trial or epoch. There would be a green symbol appears next to each component after its completion. And you can always click on each component to see its details, such as its input/output, volumes, logs, and pod.\n",
    "\n",
    "![face7](./img/face7.png)\n",
    "\n",
    "After the whole pipeline finishes running, click on Train Component and Evaluate Component, you should be able to see `main-logs` under Input/Output, Output artifacts. Click into it, you should then be able to see the detailed logs, and part of the submission, i.e. output of Evaluate Component.\n",
    "\n",
    "Example logs are provided in [logs](./logs) folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d621f",
   "metadata": {},
   "source": [
    "### Troubleshooting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
