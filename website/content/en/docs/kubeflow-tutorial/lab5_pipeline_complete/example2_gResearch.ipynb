{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ebd88b-d000-45cb-9e34-7d149c348884",
   "metadata": {},
   "source": [
    "# Lab 5: Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5d621-1e3c-4255-82e5-a5dfd6a7c324",
   "metadata": {},
   "source": [
    "### Install Kubeflow Pipeline Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38374f8b-4ffa-4b12-a3d0-440fe0f93c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277f10a-fa1f-4ace-a3e4-5caf08f3f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the kfp sdk\n",
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f6a8b-e1e8-469c-9cc0-76a99d79aba9",
   "metadata": {},
   "source": [
    "## Example 2: G-Research Crypto Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dddfa6-09d0-48d1-8fef-b00cfaf6b953",
   "metadata": {},
   "source": [
    "In this example, we would build Kubeflow Pipeline directly from the ***notebook***, without using Docker to build images first.\n",
    "\n",
    "*We assume you have already finished, or at least gone through, Example 1. If not, we recommend you to at least quickly go through Example 1, as the model in Example 1 is easier to understand and some concept explanations are discussed more in details.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce45afe-ad24-4028-a3f9-d1306a3febb5",
   "metadata": {},
   "source": [
    "### About this Model\n",
    "\n",
    "This model comes from Kaggle Competition. Over $40 billion worth of cryptocurrencies are traded every day. The objective of this task is to correctly forecast short term returns in 14 popular cryptocurrencies. [G-Research](https://www.gresearch.co.uk/) is Europe’s leading quantitative finance research firm, and partered with [Cambridge Spark](https://www.cambridgespark.com) for this Kaggle Competition.\n",
    "\n",
    "The dataset this model would use contains information on historic trades for several cryptoassets, such as Bitcoin and Ethereum. Dataset would be downloaded from Kaggle.\n",
    "\n",
    "More details about this model and dataset itself or Kaggle Competition can be found [here](https://www.kaggle.com/competitions/g-research-crypto-forecasting/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea263f9-f33d-4ced-b48f-0a26a88da100",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f66bd6-8c2f-4262-9ce3-40a1ba8edbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2a4cd-740a-48c9-a818-7373e0284cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a29952-2496-47b9-9b0b-e71a06a11966",
   "metadata": {},
   "source": [
    "### Design Pipeline\n",
    "\n",
    "Again, we design our pipeline component by component. We build our components this time as Python function-based components. The Kubeflow Pipelines SDK makes it easier to build lightweight Python function-based components by saving the effort of creating a component specification. Python function-based components make it easier to build pipeline components by building the component specification for you. Python function-based components also handle the complexity of passing inputs into your component and passing your function’s outputs back to your pipeline.\n",
    "\n",
    "Let's walk through this concrete example to understand above ideas better.\n",
    "\n",
    "Note that the overall workflow follows from original model itself. So please do not freak out when you see long, heavy code cells.\n",
    "\n",
    "Basically, the following modifications were required to the original function.\n",
    "- The import statements were moved inside of the function. Python function-based components require standalone Python functions. This means that any required import statements must be defined within the function, and any helper functions must be defined within the function. \n",
    "- The function’s arguments all include `data_path` which specifies the location for data storage, and also accessing. This lets Kubeflow Pipelines know the where to extract the data in zipped tar file into, where your function stores the processed data or model, and where to access it and use it as inputs for components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e026d8e0-72ad-42c7-84bf-1a92af0a69df",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699da23-08dc-4b02-b181-2828ec331aa3",
   "metadata": {},
   "source": [
    "We start from data downloading. As specified at the beginning of this example, the datasets we would use come from Kaggle competition.\n",
    "\n",
    "**Setup Kaggle**\n",
    "\n",
    "To enable our pipeline to download datasets from Kaggle while running, you need to first do some setups on Kaggle.\n",
    "1. Go to this [Kaggle](https://www.kaggle.com/). Login to your account, and register an account if you do not have one.\n",
    "2. Click on your user profile picture, and go to \"Account\".\n",
    "3. On your profile, Account section, scroll down and find \"API\" section. Click on \"Create New API Token\" button.\n",
    "4. Wait for a short while, a file called `kaggle.json` should be downloaded on your local computer. Open that json file. The `api-key` and `username` are the ones we would need.\n",
    "5. Keep your Kaggle account login. Go to [G-Research Crypto Forecasting Rules](https://www.kaggle.com/competitions/g-research-crypto-forecasting/rules), and click to accept those rules.\n",
    "\n",
    "OK, you're all set! The above process is completely free so there is no need to worry. For now, you need to do above works to enable our pipeline to downaload data from Kaggle. In the future, we are planning to store those datasets somewhere that would be more convinient for your to downaload from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18075-ed82-43dd-be83-9909e1c8221e",
   "metadata": {},
   "source": [
    "And we are now ready for our Download Data component design.\n",
    "\n",
    "Below is our Python-function-based Download Data component. It needs your Kaggle username and key. For convinience, you may directly copy and paste them. \n",
    "\n",
    "(But if you do not want them to be exposed directly to Kubeflow Pipeline, you may need to spend some time to create a Kubernetes secret to handle the sensitive API credentials. There are many, including the official one, [tutorials](https://kubernetes.io/docs/concepts/configuration/secret/) online, and we also provide you some sample codes in following cell. Detailed tutorial on this would come soon.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23a4f5-a64f-4c11-b444-6a9828c161b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data step\n",
    "# input dataset: name of dataset, need to be provided as input for pipeline\n",
    "# input data_path: path for data storage, need to be provided as input for pipeline\n",
    "# return: a print function, the printed contents would be reveal in logs.\n",
    "def download_data(dataset, data_path):\n",
    "        \n",
    "    # install the necessary libraries\n",
    "    # as each component would run as a container, you need to import these necessary libraries here inside this function-based component\n",
    "    import os, sys, subprocess, zipfile, pickle;\n",
    "    # again, as each component would run as a container, we use 'subprocess.run' here to run command lines while pipeline running\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','kaggle'])\n",
    "    \n",
    "    # import libraries\n",
    "    import pandas as pd\n",
    "    \n",
    "    # you may directly copy and paste your Kaggle username and key here for convinience\n",
    "    kaggle_user = 'YOUR_KAGGLE_USERNAME'\n",
    "    kaggle_key = 'YOUR_KAGGLE_KEY'\n",
    "    \n",
    "    # or, spend some time setup kaggle environment for data download\n",
    "    # with open('/secret/kaggle-secret/password', 'r') as file:\n",
    "    #     kaggle_key = file.read().rstrip()\n",
    "    # with open('/secret/kaggle-secret/username', 'r') as file:\n",
    "    #     kaggle_user = file.read().rstrip()\n",
    "        \n",
    "    os.environ['KAGGLE_USERNAME'], os.environ['KAGGLE_KEY'] = kaggle_user, kaggle_key\n",
    "    \n",
    "    # create data_path directory\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    # download kaggle's g-research-crypto-forecasting data\n",
    "    subprocess.run([\"kaggle\",\"competitions\", \"download\", \"-c\", dataset])\n",
    "    \n",
    "    # extract 'train.csv' and 'asset_details.csv' in g-research-crypto-forecasting.zip to data_path\n",
    "    with zipfile.ZipFile(f\"{dataset}.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path, members=['train.csv', 'asset_details.csv'])\n",
    "    \n",
    "    return(print('-- DATA DOWNLOADED --'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c1af3-4f5d-413e-88e4-33546484ed08",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c4cad-e00c-483e-bac9-032342ac938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data_path: path for data storage, need to be provided as input for pipeline\n",
    "# return: a print function, the printed contents would be reveal in logs.\n",
    "def load_data(data_path):\n",
    "        \n",
    "    # install the necessary libraries\n",
    "    import os, sys, subprocess, pickle;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    \n",
    "    # import libraries\n",
    "    import pandas as pd\n",
    "\n",
    "    TRAIN_CSV = f'{data_path}/train.csv'\n",
    "    ASSET_DETAILS_CSV = f'{data_path}/asset_details.csv'\n",
    "    \n",
    "    # read TRAIN_CSV and ASSET_DETAILS_CSV\n",
    "    df_train = pd.read_csv(TRAIN_CSV)\n",
    "    df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\n",
    "    \n",
    "    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n",
    "    df_train = df_train[df_train['datetime'] >= '2020-01-01 00:00:00'].copy()\n",
    "    \n",
    "    # Save the df_train data as a pickle file to be used by the feature_engineering component.\n",
    "    with open(f'{data_path}/df_train', 'wb') as f:\n",
    "        pickle.dump(df_train, f)\n",
    "        \n",
    "    # Save the df_train data as a pickle file to be used by the merge_data component.\n",
    "    with open(f'{data_path}/df_asset_details', 'wb') as g:\n",
    "        pickle.dump(df_asset_details, g)\n",
    "\n",
    "    \n",
    "    return(print('-- DATA LOADED --'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392cb7c-01aa-4441-908d-368f41256868",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "Codes for this step follows most from original model itself (so don't freak out when seeing this long cell).\n",
    "\n",
    "One thing to note here is that this component needs to save the feature engineered data as a pickle file which would be used later by the modeling component. Similar to Example 1, to enable modeling component to access this feature engineered data, we simply need to store it in the data path.\n",
    "```python\n",
    "# save the feature engineered data as a pickle file to be used by the modeling component.\n",
    "    with open(f'{data_path}/feature_df', 'wb') as f:\n",
    "        pickle.dump(feature_df, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19551a3-43ce-4cb0-a38c-bd4060a5fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data_path: path for data storage, need to be provided as input for pipeline\n",
    "# return: a print function, the printed contents would be reveal in logs.\n",
    "def feature_engineering(data_path):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','tqdm'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','talib-binary'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, pickle, time, talib, datetime;\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # loading the df_train data\n",
    "    with open(f'{data_path}/df_train', 'rb') as f:\n",
    "        df_train = pickle.load(f)\n",
    "    \n",
    "    # creating technical indicators\n",
    "    \n",
    "    # Create a function to calculate the Relative Strength Index\n",
    "    def RSI(df, n):\n",
    "        return talib.RSI(df['Close'], n)\n",
    "    \n",
    "    # Create a function to calculate the Average True Range\n",
    "    def ATR(df, n):\n",
    "        return talib.ATR(df[\"High\"], df.Low, df.Close, n)\n",
    "\n",
    "    # Create a function to calculate the Double Exponential Moving Average (DEMA)\n",
    "    def DEMA(data, time_period):\n",
    "        #Calculate the Exponential Moving Average for some time_period (in days)\n",
    "        EMA = data['Close'].ewm(span=time_period, adjust=False).mean()\n",
    "        #Calculate the DEMA\n",
    "        DEMA = 2*EMA - EMA.ewm(span=time_period, adjust=False).mean()\n",
    "        return DEMA\n",
    "    \n",
    "    # Create a function to calculate the upper_shadow\n",
    "    def upper_shadow(df):\n",
    "        return df['High'] - np.maximum(df['Close'], df['Open'])\n",
    "    \n",
    "    # Create a function to calculate the lower_shadow\n",
    "    def lower_shadow(df):\n",
    "        return np.minimum(df['Close'], df['Open']) - df['Low']\n",
    "    \n",
    "    \n",
    "    def get_features(df, asset_id, train=True):\n",
    "        '''\n",
    "        This function takes a dataframe with all asset data and return the lagged features for a single asset.\n",
    "\n",
    "        df - Full dataframe with all assets included\n",
    "        asset_id - integer from 0-13 inclusive to represent a cryptocurrency asset\n",
    "        train - True - you are training your model\n",
    "              - False - you are submitting your model via api\n",
    "        '''\n",
    "        # filter based on asset id\n",
    "        df = df[df['Asset_ID']==asset_id]\n",
    "\n",
    "        # sort based on time stamp\n",
    "        df = df.sort_values('timestamp')\n",
    "\n",
    "        if train == True:\n",
    "            df_feat = df.copy()\n",
    "\n",
    "            # define a train_flg column to split your data into train and validation\n",
    "            totimestamp = lambda s: np.int32(time.mktime(datetime.datetime.strptime(s, \"%d/%m/%Y\").timetuple()))\n",
    "            valid_window = [totimestamp(\"01/05/2021\")]\n",
    "\n",
    "            df_feat['train_flg'] = np.where(df_feat['timestamp']>=valid_window[0], 0,1)\n",
    "            df_feat = df_feat[['timestamp','Asset_ID', 'High', 'Low', 'Open', 'Close', 'Volume','Target','train_flg']].copy()\n",
    "        else:\n",
    "            df = df.sort_values('row_id')\n",
    "            df_feat = df[['Asset_ID', 'High', 'Low', 'Open', 'Close', 'Volume','row_id']].copy()\n",
    "\n",
    "        for i in tqdm([30, 120, 240]):\n",
    "            # Applyin technical indicators\n",
    "            df_feat[f'RSI_{i}'] = RSI(df_feat, i)\n",
    "            df_feat[f'ATR_{i}'] = ATR(df_feat, i)\n",
    "            df_feat[f'DEMA_{i}'] = DEMA(df_feat, i)\n",
    "\n",
    "        for i in tqdm([30, 120, 240]):\n",
    "            # creating lag features\n",
    "            df_feat[f'sma_{i}'] = df_feat['Close'].rolling(i).mean()/df_feat['Close'] -1\n",
    "            df_feat[f'return_{i}'] = df_feat['Close']/df_feat['Close'].shift(i) -1\n",
    "\n",
    "        # new features\n",
    "        df_feat['HL'] = np.log(df_feat['High'] - df_feat['Low'])\n",
    "        df_feat['OC'] = np.log(df_feat['Close'] - df_feat['Open'])\n",
    "        \n",
    "        # Applyin lower_shadow and upper_shadow indicators\n",
    "        df_feat['lower_shadow'] = np.log(lower_shadow(df)) \n",
    "        df_feat['upper_shadow'] = np.log(upper_shadow(df))\n",
    "\n",
    "        # replace inf with nan\n",
    "        df_feat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # datetime features\n",
    "        df_feat['Date'] = pd.to_datetime(df_feat['timestamp'], unit='s')\n",
    "        df_feat['Day'] = df_feat['Date'].dt.weekday.astype(np.int32)\n",
    "        df_feat[\"dayofyear\"] = df_feat['Date'].dt.dayofyear\n",
    "        df_feat[\"weekofyear\"] = df_feat['Date'].dt.weekofyear\n",
    "        df_feat[\"season\"] = ((df_feat['Date'].dt.month)%12 + 3)//3\n",
    "        \n",
    "        # drop features\n",
    "        df_feat = df_feat.drop(['Open','Close','High','Low', 'Volume', 'Date'], axis=1)\n",
    "\n",
    "        # fill nan values with 0\n",
    "        df_feat = df_feat.fillna(0)\n",
    "\n",
    "        return df_feat\n",
    "    \n",
    "    # create your features dataframe for each asset and concatenate\n",
    "    feature_df = pd.DataFrame()\n",
    "    for i in range(14):\n",
    "        print(i)\n",
    "        feature_df = pd.concat([feature_df,get_features(df_train,i,train=True)])\n",
    "      \n",
    "    # save the feature engineered data as a pickle file to be used by the modeling component.\n",
    "    with open(f'{data_path}/feature_df', 'wb') as f:\n",
    "        pickle.dump(feature_df, f)\n",
    "    \n",
    "    return(print('-- FEATURE ENGINEERING FINISHED --')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a91898-803c-4783-b970-03f1547b69bf",
   "metadata": {},
   "source": [
    "#### Merge Assets Features\n",
    "\n",
    "This component needs to access the featured data generated in above Feature Engineering component. That data is stored in the data path we specified.\n",
    "```python\n",
    "with open(f'{data_path}/feature_df', 'rb') as f:\n",
    "        feature_df = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0476d1-0b59-4d10-ba50-967636e77f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data_path: path for data storage, need to be provided as input for pipeline\n",
    "# return: a print function, the printed contents would be reveal in logs.\n",
    "def merge_assets_features(data_path):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "\n",
    "    #loading the feature_df data\n",
    "    with open(f'{data_path}/feature_df', 'rb') as f:\n",
    "        feature_df = pickle.load(f)\n",
    "        \n",
    "    #loading the df_asset_details data\n",
    "    with open(f'{data_path}/df_asset_details', 'rb') as g:\n",
    "        df_asset_details = pickle.load(g)\n",
    "    \n",
    "    # assign weight column feature dataframe\n",
    "    feature_df = pd.merge(feature_df, df_asset_details[['Asset_ID','Weight']], how='left', on=['Asset_ID'])\n",
    "\n",
    "    #Save the feature_df as a pickle file to be used by the modelling component.\n",
    "    with open(f'{data_path}/merge_feature_df', 'wb') as h:\n",
    "        pickle.dump(feature_df, h)\n",
    "        \n",
    "    return(print('-- ASSETS FEATURES MERGED --'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6628c4-dcc8-49a8-b8f1-8ecc61de34f5",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "Again, the overall logic comes completely from the original ML model. Things we need to note here, for our pipeline lab, is, again, the location of data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573a1aa-ca58-42a6-81f8-7c00e2856225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data_path: path for data storage, need to be provided as input for pipeline\n",
    "# return: a print function, the printed contents would be reveal in logs.\n",
    "def modeling(data_path):\n",
    "    \n",
    "    # install the necessary libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','lightgbm'])\n",
    "    \n",
    "    # import Library\n",
    "    import os, pickle, joblib;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "    #loading the new_feats data\n",
    "    with open(f'{data_path}/merge_feature_df', 'rb') as f:\n",
    "        feature_df = pickle.load(f)\n",
    "        \n",
    "    # define features for LGBM\n",
    "    features = ['Asset_ID', 'RSI_30', 'ATR_30',\n",
    "           'DEMA_30', 'RSI_120', 'ATR_120', 'DEMA_120', 'RSI_240', 'ATR_240',\n",
    "           'DEMA_240', 'sma_30', 'return_30', 'sma_120', 'return_120', 'sma_240',\n",
    "           'return_240', 'HL', 'OC', 'lower_shadow', 'upper_shadow', 'Day',\n",
    "           'dayofyear', 'weekofyear', 'season']\n",
    "    categoricals = ['Asset_ID']\n",
    "    \n",
    "    # define the evaluation metric\n",
    "    def weighted_correlation(a, train_data):\n",
    "\n",
    "        weights = train_data.add_w.values.flatten()\n",
    "        b = train_data.get_label()\n",
    "\n",
    "\n",
    "        w = np.ravel(weights)\n",
    "        a = np.ravel(a)\n",
    "        b = np.ravel(b)\n",
    "\n",
    "        sum_w = np.sum(w)\n",
    "        mean_a = np.sum(a * w) / sum_w\n",
    "        mean_b = np.sum(b * w) / sum_w\n",
    "        var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "        var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "\n",
    "        cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "        corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "        return 'eval_wcorr', corr, True\n",
    "    \n",
    "    # define train and validation weights and datasets\n",
    "    weights_train = feature_df.query('train_flg == 1')[['Weight']]\n",
    "    weights_test = feature_df.query('train_flg == 0')[['Weight']]\n",
    "\n",
    "    train_dataset = lgb.Dataset(feature_df.query('train_flg == 1')[features], \n",
    "                                feature_df.query('train_flg == 1')['Target'].values, \n",
    "                                feature_name = features,\n",
    "                               categorical_feature= categoricals)\n",
    "    val_dataset = lgb.Dataset(feature_df.query('train_flg == 0')[features], \n",
    "                              feature_df.query('train_flg == 0')['Target'].values, \n",
    "                              feature_name = features,\n",
    "                             categorical_feature= categoricals)\n",
    "    # add weights\n",
    "    train_dataset.add_w = weights_train\n",
    "    val_dataset.add_w = weights_test\n",
    "    \n",
    "    # LGBM params\n",
    "    evals_result = {}\n",
    "    params = {'n_estimators': 1200,\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'max_depth': -1, \n",
    "            'learning_rate': 0.01,\n",
    "            'seed': 2022,\n",
    "            'verbose': -1,\n",
    "            }\n",
    "\n",
    "    # train LGBM\n",
    "    model = lgb.train(params = params,\n",
    "                      train_set = train_dataset, \n",
    "                      valid_sets = [val_dataset],\n",
    "                      early_stopping_rounds=60,\n",
    "                      verbose_eval = 30,\n",
    "                      feval=weighted_correlation,\n",
    "                      evals_result = evals_result \n",
    "                     )\n",
    "    \n",
    "    # saving model\n",
    "    joblib.dump(model, f'{data_path}/lgb.jl')\n",
    "        \n",
    "    return(print('-- MODELED --'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db5cc3-8fd7-4a07-bf0c-8d2fabe924a5",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "This component would return the evaluation metrics, which would be reveal in Input/Output, Output artifacts on Kubeflow UI after pipeline finishes running. More details on this would be discussed later in pipeline running section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cace99-aaee-4bc4-b8cb-a1294602a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data_path: path for data storage, need to be provided as input for pipeline\n",
    "# input metric_path: path to store evaluation metric\n",
    "# return the evaluation metrics, which would be reveal in Input/Output, Output artifacts\n",
    "def evaluation_result(data_path, \n",
    "                metrics_path: OutputPath(str)) -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','lightgbm'])\n",
    "    import json;\n",
    "    from collections import namedtuple\n",
    "    import joblib\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMRegressor\n",
    "    \n",
    "    # load model\n",
    "    model = joblib.load(f'{data_path}/lgb.jl')\n",
    "\n",
    "    # model evaluation\n",
    "    root_mean_squared_error = model.best_score.get('valid_0').get('rmse')\n",
    "    weighted_correlation = model.best_score.get('valid_0').get('eval_wcorr')\n",
    "    \n",
    "    # create kubeflow metric metadata for UI    \n",
    "    metrics = {\n",
    "                'metrics': [\n",
    "                    {'name': 'root-mean-squared-error',\n",
    "                    'numberValue':  root_mean_squared_error,\n",
    "                    'format': 'RAW'},\n",
    "                    {'name': 'weighted-correlation',\n",
    "                    'numberValue':  weighted_correlation,\n",
    "                    'format': 'RAW'}\n",
    "                            ]\n",
    "              }\n",
    "    \n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    output_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return output_tuple(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48aa66b-9dc6-4b31-a493-1ec5ea7431d1",
   "metadata": {},
   "source": [
    "### Create Pipeline Components\n",
    "\n",
    "In this example, we use `kfp.components.create_component_from_func` to return a factory function that we would use to create pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc42b9-10d8-49e0-8074-016bc9c37dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = comp.create_component_from_func(download_data,base_image=\"python:3.7.1\")\n",
    "load_op = comp.create_component_from_func(load_data,base_image=\"python:3.7.1\")\n",
    "merge_assets_features_op = comp.create_component_from_func(merge_assets_features,base_image=\"python:3.7.1\")\n",
    "feature_eng_op = comp.create_component_from_func(feature_engineering,base_image=\"python:3.7.1\")\n",
    "modeling_op = comp.create_component_from_func(modeling, base_image=\"python:3.7.1\")\n",
    "evaluation_op = comp.create_component_from_func(evaluation_result, base_image=\"python:3.7.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38787f9-d11c-49bf-af9a-44854e8bdf44",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3c131-6859-481f-ac5b-ff0b90b57314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"g-research-crypto-forecasting-pipeline\", \n",
    "              description=\"g-research-crypto-forecasting-pipeline\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def g_research_crypto_forecast_pipeline(\n",
    "                             dataset: str, # name of dataset, zip\n",
    "                             data_path: str # path for data storage, would be used through entire pipeline, all components\n",
    "                            ):\n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_data_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"16Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    \n",
    "    # Create download container, using ops we defined in above code cell\n",
    "    download_container = download_op(dataset, data_path)\\\n",
    "                        .add_pvolumes({data_path: vop.volume}) # add pvolumes, and mount it to data_path for data sharing among steps\n",
    "    # Create load container.\n",
    "    load_container = load_op(data_path)\\\n",
    "                    .add_pvolumes({data_path: download_container.pvolume})\n",
    "    # Create feature engineering container.\n",
    "    feat_eng_container = feature_eng_op(data_path)\\\n",
    "                            .add_pvolumes({data_path: load_container.pvolume})\n",
    "    # Create merge_assets_feat container.\n",
    "    merge_assets_feat_container = merge_assets_features_op(data_path)\\\n",
    "                                 .add_pvolumes({data_path: feat_eng_container.pvolume})\n",
    "    # Create modeling container.\n",
    "    modeling_container = modeling_op(data_path)\\\n",
    "                        .add_pvolumes({data_path: merge_assets_feat_container.pvolume})\n",
    "    # Create prediction container.\n",
    "    evaluation_container = evaluation_op(data_path).add_pvolumes({data_path: modeling_container.pvolume})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2ad5e-d32e-4fe5-89bc-caf72fc8bb03",
   "metadata": {},
   "source": [
    "Similar to Example 1, compile our difined pipeline into a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d1ed6-e108-4b61-a058-92fef37f02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=g_research_crypto_forecast_pipeline,\n",
    "    package_path='pipeline_gResearch.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3aef5f-237c-469c-824f-ff28fd24df06",
   "metadata": {},
   "source": [
    "You should then be able to see a file called `pipeline_gResearch.yaml` in your current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdef551-3415-4883-89ef-bf30275f2bed",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6490ba-06ac-4815-9086-98eff1ae8db5",
   "metadata": {},
   "source": [
    "Again, we provide you with a compiled pipeline YAML file `gResearch_pipeline_test.yaml`. Feel free to use it.\n",
    "\n",
    "Similar to Example 1, download `pipeline_gResearch.yaml`, and upload it to Pipelines on Kubeflow UI.\n",
    "\n",
    "![g1](./img/g1.png)\n",
    "\n",
    "![g2](./img/g2.png)\n",
    "\n",
    "![g3](./img/g3.png)\n",
    "\n",
    "This pipeline graph is more complex than the one in Example 1. You may take some time going through it. \n",
    "\n",
    "Again, create an experiment for this pipeline, and create a run. This time, you need to provide two inputs, `dataset` and `data_path`, exactly the ones for our first step Data Download. If you do not intend to make any personalization on datasets and data path, enter following values\n",
    "```python\n",
    "# arguments\n",
    "# \"\" not needed while inputting them\n",
    "dataset = \"g-research-crypto-forecasting\"\n",
    "data_path = \"/mnt\"\n",
    "```\n",
    "\n",
    "![g4](./img/g4.png)\n",
    "\n",
    "![g5](./img/g5.png)\n",
    "\n",
    "The pipeline would start to run then. You would be able to see the running process in Runs Page on Kubeflow UI.\n",
    "\n",
    "![g6](./img/g6.png)\n",
    "\n",
    "The pipeline running may take some time, as the datasets is pretty big and there are much more steps in this example. There would be a green symbol appears next to each component after its completion. And you can always click on each component to see its details, such as its input/output, volumes, logs, and pod.\n",
    "\n",
    "![g7](./img/g7.png)\n",
    "\n",
    "After the whole pipeline finishes running, click on any of the component/step that you are interested in. You should be able to see main-logs under Input/Output, Output artifacts. \n",
    "\n",
    "Specifically, click on Evaluation Result Step, you should see \"metrics\" under Input/Output, Output artifacts. Click into them to see the evaluation metrics.\n",
    "\n",
    "We also provide you with example logs of evaluation metrics in [logs](./logs/) folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98649ac2-fa6d-446a-ba28-fdedc500e022",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce4a1f2401046e759c80303d07fb2b71c902d4161a17daad675bac69d9d8e0a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
