{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b667d1-fe3d-48e6-ad1f-3dc5ef285274",
   "metadata": {},
   "source": [
    "# bloom_kserve\n",
    "Deploy PyTorch model with TorchServe InferenceService\n",
    "\n",
    "If you want to know more detailes, please refer to https://github.com/kserve/kserve/tree/master/docs/samples/v1beta1/torchserve/v1/bloom. \n",
    "\n",
    "This tutorial is that study the above example and improve it so that can run inferenceservice in the Kubeflow on vSphere environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799dfe46-4a37-4d1f-95f0-bfcf9b5b8fae",
   "metadata": {},
   "source": [
    "## 1. Prepare MAR model package and it's config\n",
    "\n",
    "Clone https://github.com/AmyHoney/bloom.git repository and follow the README.md and bloom_torchserve.ipynb for creating the MAR file including serialized model, other dependent files and configuration.\n",
    "\n",
    "Study the example https://github.com/pytorch/serve/tree/master/examples/large_models/Huggingface_accelerate and improve it so that it can run in the kubeflow on vSphere environment\n",
    "The above Torchserve example works on shard version of Huggingface models.\n",
    "For sharding the Huggingface models you can use the following script, and then compress the model\n",
    "\n",
    "**In this tutorial we will show how to serve Large Huggingface models(Bloom) with TorchServe on KServe.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eac9c6-bd87-49d9-82f1-63308dcfb24d",
   "metadata": {},
   "source": [
    "**Notice:** Install requirments from tsinghua source as below if your network without VPN when you run step 3: generate MAR file, Or got error: ImportError: cannot import name 'BloomForCausalLM' from 'transformers' and ModuleNotFoundError: No module named 'ts.torch_handler.custom_handler'\n",
    "\n",
    "**Besides**, there use bloom-560m to do inferenceservice due to resources limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dde4fbc-a793-485f-8e5b-50b58e2caf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.25.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "accelerate==0.18.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "pynvml==8.0.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c65960-ab1d-4a84-a762-d2162367c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\tmodel-store  sample_text4.json\ttest.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9083aca-56a7-4e14-b585-1711411d5656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom.mar\n",
      "config.properties\n"
     ]
    }
   ],
   "source": [
    "# check have prepared the MAR model and it's config\n",
    "!ls ./model-store/\n",
    "!ls ./config/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4932f41e-d2d1-46c2-b2d6-7a44595bcc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_address=http://0.0.0.0:8085\n",
      "management_address=http://0.0.0.0:8085\n",
      "metrics_address=http://0.0.0.0:8082\n",
      "grpc_inference_port=7070\n",
      "grpc_management_port=7071\n",
      "enable_metrics_api=true\n",
      "metrics_format=prometheus\n",
      "number_of_netty_threads=4\n",
      "job_queue_size=10\n",
      "enable_envvars_config=true\n",
      "install_py_dep_per_model=true\n",
      "model_store=/mnt/models/model-store\n",
      "model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":1,\"models\":{\"bloom\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"bloom.mar\",\"minWorkers\":1,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":5000,\"responseTimeout\":120}}}}"
     ]
    }
   ],
   "source": [
    "!cat ./config/config.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823f5e0-9224-479c-ad0e-0be4041cd62c",
   "metadata": {},
   "source": [
    "## 2. Create PVC and Mount the Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324587a5-2daa-4f14-ad2e-bf5cddee03c8",
   "metadata": {},
   "source": [
    "### 2.1 Create a volume in Kubeflow UI\n",
    "\n",
    "Use your web browser to login to Kubeflow, and click volumes, then click the `New volume` in the top right bar to create pvc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa6417-f19e-4f8a-9719-bddd2eaf2050",
   "metadata": {},
   "source": [
    "![voumes creating](./img/volumes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155cc7eb-3619-488d-a377-f98fc14ea599",
   "metadata": {},
   "source": [
    "### 2.2 Create pv-pod to mount the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f97db6c-3691-48cc-9589-f1236b287d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/model-store-pod created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF | kubectl create -n zyajing -f -\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: model-store-pod\n",
    "spec:\n",
    "  volumes:\n",
    "    - name: pv-storage\n",
    "      persistentVolumeClaim:\n",
    "        claimName: model-local-claim\n",
    "  containers:\n",
    "    - name: pv-container\n",
    "      image: ubuntu\n",
    "      command: [ \"sleep\" ]\n",
    "      args: [ \"infinity\" ]\n",
    "      volumeMounts:\n",
    "        - mountPath: \"/pv\"\n",
    "          name: pv-storage\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: \"4Gi\"\n",
    "          cpu: \"2\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33dc2d-5765-44a6-a6c6-65ac221c13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the config.properties and MAR file to PVC\n",
    "\n",
    "!kubectl exec -it model-store-pod -n zyajing -- cd pv/ && mkdir model_store\n",
    "!kubectl exec -it model-store-pod -n zyajing -- cd pv/ && mkdir config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c8eed-bf7b-4f78-aeed-3a68d23999c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec -it model-store-pod -n zyajing -- ls -al /pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc424e6-daf5-471d-8952-d952dc5e3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to copy\n",
    "!kubectl cp bloom.mar model-store-pod:/pv/model-store/ -n zyajing\n",
    "!kubectl cp config.properties model-store-pod:/pv/config/ -n zyajing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a55a2-11db-4897-b047-f307282e4d27",
   "metadata": {},
   "source": [
    "## 3. Create the InferenceService\n",
    "\n",
    "**Notice:** Remember to delete `model-store-pod` pod to unmount pvc, Or if you create inferenceservice YAML file, will got `model-local-claim` pvc have been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c56591-8cca-44ed-abc3-2bc66f1e7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/torchserve-bloom-560mm created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Apply the CRD\n",
    "# storageUri: \"pvc://${PVC_NAME}/model.joblib\"\n",
    "cat << EOF | kubectl create -n zyajing -f -\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: \"torchserve-bloom-560mm\"\n",
    "spec:\n",
    "  predictor:\n",
    "    pytorch:\n",
    "      storageUri: pvc://model-local-claim\n",
    "      resources:\n",
    "          limits:\n",
    "            cpu: 8\n",
    "            memory: 16Gi\n",
    "            nvidia.com/gpu: 1\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501314a1-452d-447e-9806-f4c27312d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs \n",
    "# kubectl logs <pod_name> -n zyajing\n",
    "kubectl logs torchserve-bloom-560m-predictor-default-00001-deployment-876shh -n zyajing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a5964-df4e-420a-aa91-f2dc3f6c3d01",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a97da-a928-4938-94e9-2ce6c2ac19e4",
   "metadata": {},
   "source": [
    "### 4.1 check GPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07d40372-c829-406a-9197-ad2fcc7bf497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:zyajing:default-editor\" cannot list resource \"pods\" in API group \"\" at the cluster scope\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -A | grep dcgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef49477-740e-4a4c-83d7-c319a0529035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run the below command, got Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:zyajing:default-editor\" cannot list resource \"pods\" in API group \"\" at the cluster scope\n",
    "# Run the below command in the terminal sucessfully.\n",
    "$ kubectl get po -n zyajing -o wide\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE     IP           NODE                                                     NOMINATED NODE   READINESS GATES\n",
    "torchserve-bloom-560m-predictor-default-00001-deployment-876shh   3/3     Running   0          86m     192.0.7.55   liuqi-test-vgpu-v100-16c-2-np-2-8bfsx-f694667c-pc8wr     <none>           <none>\n",
    "\n",
    "!kubectl get pods -A | grep dcgm\n",
    "kubectl get pods -A -o wide| grep dcgm | grep pc8wr\n",
    "gpu-operator                   nvidia-dcgm-exporter-9pk2w                                               1/1     Running            0                  4d18h   192.0.7.6     liuqi-test-vgpu-v100-16c-2-np-2-8bfsx-f694667c-pc8wr     <none>           <none>\n",
    "\n",
    "$ kubectl exec -n gpu-operator nvidia-dcgm-exporter-9pk2w -- nvidia-smi\n",
    "Defaulted container \"nvidia-dcgm-exporter\" out of: nvidia-dcgm-exporter, toolkit-validation (init)\n",
    "Tue Apr 25 03:06:45 2023\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  GRID V100-16C       On   | 00000000:02:00.0 Off |                    0 |\n",
    "| N/A   N/A    P0    N/A /  N/A |   1834MiB / 16384MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "+-----------------------------------------------------------------------------+\n",
    "\n",
    "# It seems that the gpu resources have been occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5523ec5-d354-4b02-8ce7-8542bb9e4909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4990f0b2-69e2-4694-afca-d1fbf264cf6b",
   "metadata": {},
   "source": [
    "## 3.2 Option 1 curl to access model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d418c392-807a-4a4c-a874-1664b1aa85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchserve-bloom-560m.zyajing.example.com\n"
     ]
    }
   ],
   "source": [
    "# !export MODEL_NAME=bloom\n",
    "# !export ISVC_NAME=torchserve-bloom-560m \n",
    "!kubectl get inferenceservice torchserve-bloom-560m -n zyajing -o jsonpath='{.status.url}' | cut -d \"/\" -f 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66093b-a8b9-49c8-860f-5fe0ef658f0a",
   "metadata": {},
   "source": [
    "Use your web browser to login to Kubeflow, and get Cookies: authservice_session (Chrome: Developer Tools -> Applications -> Cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bca2598a-d165-4d88-a501-b99222f9ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export SERVICE_HOSTNAME='torchserve-bloom-560m.zyajing.example.com'\n",
    "!export SESSION='MTY4MjM5Mzc1N3xOd3dBTkRWUldsRTNSMFpNVmxOR1ZqWk5XRE5GVkZNMVdVbEVOVW8yTnpaRlJscEpXazVhVkZvME4xQlFTek5IVWtaWlQwZEZTbEU9fEgO5cAxqnPfp9k9VVne7NGpI-7kHEKgYo0BqlnUFbtI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21957717-2ac3-4075-abf8-253cf57dfbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchserve-bloom-560m.zyajing.example.com\n",
      "MTY4MjMwNzMxMHxOd3dBTkVSUVFUWkpNMHBPV0ZWUVR6SlpXRE5XTmxSWU1rbFlSVXBQVlVjMFVVTXpXVVpTVjFCQ01rczFOelpXVEZwSVYweFdORkU9fJSm63ckzmJ0ZzAS1qpgi-NcY-tuPLBuK9CIWPd_DmP1\n"
     ]
    }
   ],
   "source": [
    "!echo $SERVICE_HOSTNAME\n",
    "!echo $SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2784f602-586b-4cbc-95d3-5059ade3ed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.105.150.44:80...\n",
      "* TCP_NODELAY set\n",
      "* Connected to 10.105.150.44 (10.105.150.44) port 80 (#0)\n",
      "> POST /v1/models/bloom:predict HTTP/1.1\n",
      "> Host: \n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> Cookie: authservice_session=\n",
      "> Content-Length: 61\n",
      "> Content-Type: application/x-www-form-urlencoded\n",
      "> \n",
      "* upload completely sent off: 61 out of 61 bytes\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 403 Forbidden\n",
      "< date: Tue, 25 Apr 2023 04:11:09 GMT\n",
      "< server: envoy\n",
      "< content-length: 0\n",
      "< x-envoy-upstream-service-time: 4\n",
      "< \n",
      "* Connection #0 to host 10.105.150.44 left intact\n"
     ]
    }
   ],
   "source": [
    "# The first step is refer https://kserve.github.io/website/0.10/get_started/first_isvc/#4-determine-the-ingress-ip-and-ports to determine the ingress IP and ports and set INGRESS_HOSTï¼š10.105.150.44 and INGRESS_PORT:80\n",
    "!curl -v -H \"Host: $SERVICE_HOSTNAME\" -H \"Cookie: authservice_session=$SESSION\" http://10.105.150.44:80/v1/models/bloom:predict -d @./sample_text4.json\n",
    "# From the below log, there is no find Host and authservice_session, even though I have exported Host and authservice_session before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105b23e7-60d9-4a1a-aa80-9a5ecdbaf376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.105.150.44:80...\n",
      "* TCP_NODELAY set\n",
      "* Connected to 10.105.150.44 (10.105.150.44) port 80 (#0)\n",
      "> POST /v1/models/bloom:predict HTTP/1.1\n",
      "> Host: torchserve-bloom-560m.zyajing.example.com\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> Cookie: authservice_session=MTY4MjM5Mzc1N3xOd3dBTkRWUldsRTNSMFpNVmxOR1ZqWk5XRE5GVkZNMVdVbEVOVW8yTnpaRlJscEpXazVhVkZvME4xQlFTek5IVWtaWlQwZEZTbEU9fEgO5cAxqnPfp9k9VVne7NGpI-7kHEKgYo0BqlnUFbtI\n",
      "> Content-Length: 61\n",
      "> Content-Type: application/x-www-form-urlencoded\n",
      "> \n",
      "* upload completely sent off: 61 out of 61 bytes\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< content-length: 372\n",
      "< content-type: application/json; charset=UTF-8\n",
      "< date: Wed, 26 Apr 2023 03:14:19 GMT\n",
      "< server: envoy\n",
      "< x-envoy-upstream-service-time: 2970\n",
      "< \n",
      "* Connection #0 to host 10.105.150.44 left intact\n",
      "{\"predictions\": [\"My dog is cute and smart and kind. He loves to be around and will be an easy companion!\\nCotton is the perfect material for a wall with a wooden base. It can be fitted with paint, and a pattern or line can be added to the wall. You can also add a colourful flower.\\nCotton wall siding can help prevent water ingress from damp. It gives you a good wind\"]}"
     ]
    }
   ],
   "source": [
    "# use \"-d @./sample_text.json\" to curl, Or if you \"-d ./sample_text.txt\", will got error \"400: Unrecognized request format: Expecting value: line 1 column 1 (char 0)\"\n",
    "!curl -v -H \"Host: torchserve-bloom-560m.zyajing.example.com\" -H \"Cookie: authservice_session=MTY4MjM5Mzc1N3xOd3dBTkRWUldsRTNSMFpNVmxOR1ZqWk5XRE5GVkZNMVdVbEVOVW8yTnpaRlJscEpXazVhVkZvME4xQlFTek5IVWtaWlQwZEZTbEU9fEgO5cAxqnPfp9k9VVne7NGpI-7kHEKgYo0BqlnUFbtI\" http://10.105.150.44:80/v1/models/bloom:predict -d @./sample_text.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4aaeb-dd36-41fd-9ff2-d6627672aebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
