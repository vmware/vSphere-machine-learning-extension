{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4689bda-a7ca-4568-bdfc-6c974dbda944",
   "metadata": {},
   "source": [
    "# nanoGPT\n",
    "\n",
    "This notebook is based on https://github.com/karpathy/nanoGPT.\n",
    "\n",
    "The **reproducing GPT-2** part is not taken as it requires too much resources and takes long time.\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1bf8c3-a9ce-4ceb-8cdb-bd50232a8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \n",
      "Hit:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease           \n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]      \n",
      "Hit:6 https://deb.nodesource.com/node_14.x focal InRelease                     \n",
      "Fetched 222 kB in 2s (94.4 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 149 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get -y install gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65546628-2f00-49fb-9b09-0bbcb2b506c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0 in /opt/conda/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (1.20.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.14.4)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.8/site-packages (0.15.8)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (4.66.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (3.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (2.7.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (3.10.0.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.7.99)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch==2.0) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0) (0.36.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0) (3.27.2)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0) (16.0.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.8/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.8/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.29.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch==2.0) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch==2.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.0 numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887e6a06-4348-4ea5-b0cf-2bdacbc511e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanoGPT'...\n",
      "remote: Enumerating objects: 649, done.\u001b[K\n",
      "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
      "Receiving objects: 100% (649/649), 935.29 KiB | 1.47 MiB/s, done.\n",
      "Resolving deltas: 100% (374/374), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf nanoGPT && git clone https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37add33-6d66-4e70-ba8c-cde0396d3e5e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e097780-177b-4839-9b58-a3fde8cf1dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "cd nanoGPT/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e36656-625b-4922-8f8b-a09c5908b258",
   "metadata": {},
   "source": [
    "Download the data as a single (1MB) file and turn it from raw text into one large stream of integers. This creates a train.bin and val.bin in that data/shakespeare_char directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27deb14a-c3b1-4381-aece-043850f1dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3647bc-7ed1-4765-8351-8a1fa1893bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.txt  meta.pkl  prepare.py  readme.md  train.bin  val.bin\n"
     ]
    }
   ],
   "source": [
    "ls data/shakespeare_char/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3d4134-065f-4951-9674-c44b6166159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "tokens per iteration will be: 16,384\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 10.65M\n",
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 4.2874, val loss 4.2823\n",
      "[2023-08-25 06:25:09,340] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 4.2714, time 20349.70ms, mfu -100.00%\n",
      "iter 10: loss 3.2442, time 35.27ms, mfu 10.56%\n",
      "iter 20: loss 2.7949, time 35.09ms, mfu 10.57%\n",
      "iter 30: loss 2.6398, time 35.12ms, mfu 10.57%\n",
      "iter 40: loss 2.5768, time 35.10ms, mfu 10.58%\n",
      "iter 50: loss 2.5239, time 34.98ms, mfu 10.59%\n",
      "iter 60: loss 2.5148, time 35.17ms, mfu 10.59%\n",
      "iter 70: loss 2.4940, time 35.10ms, mfu 10.59%\n",
      "iter 80: loss 2.4973, time 35.26ms, mfu 10.59%\n",
      "iter 90: loss 2.4809, time 35.21ms, mfu 10.59%\n",
      "iter 100: loss 2.4585, time 35.07ms, mfu 10.59%\n",
      "iter 110: loss 2.4516, time 35.15ms, mfu 10.59%\n",
      "iter 120: loss 2.4394, time 35.15ms, mfu 10.59%\n",
      "iter 130: loss 2.4186, time 35.15ms, mfu 10.59%\n",
      "iter 140: loss 2.4009, time 35.00ms, mfu 10.60%\n",
      "iter 150: loss 2.4227, time 34.96ms, mfu 10.60%\n",
      "iter 160: loss 2.3714, time 34.98ms, mfu 10.61%\n",
      "iter 170: loss 2.3553, time 35.03ms, mfu 10.61%\n",
      "iter 180: loss 2.3134, time 35.09ms, mfu 10.61%\n",
      "iter 190: loss 2.2374, time 34.96ms, mfu 10.62%\n",
      "iter 200: loss 2.2104, time 35.24ms, mfu 10.61%\n",
      "iter 210: loss 2.1484, time 35.07ms, mfu 10.61%\n",
      "iter 220: loss 2.1388, time 35.06ms, mfu 10.62%\n",
      "iter 230: loss 2.0770, time 35.24ms, mfu 10.61%\n",
      "iter 240: loss 2.0809, time 35.21ms, mfu 10.61%\n",
      "step 250: train loss 1.9610, val loss 2.0651\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.0330, time 4684.66ms, mfu 9.56%\n",
      "iter 260: loss 1.9806, time 35.29ms, mfu 9.66%\n",
      "iter 270: loss 1.9724, time 35.08ms, mfu 9.75%\n",
      "iter 280: loss 1.9830, time 35.27ms, mfu 9.83%\n",
      "iter 290: loss 1.9260, time 35.39ms, mfu 9.90%\n",
      "iter 300: loss 1.9052, time 35.11ms, mfu 9.97%\n",
      "iter 310: loss 1.8673, time 35.24ms, mfu 10.03%\n",
      "iter 320: loss 1.8603, time 35.32ms, mfu 10.09%\n",
      "iter 330: loss 1.8236, time 35.06ms, mfu 10.14%\n",
      "iter 340: loss 1.7837, time 35.06ms, mfu 10.19%\n",
      "iter 350: loss 1.8342, time 35.06ms, mfu 10.23%\n",
      "iter 360: loss 1.7644, time 35.19ms, mfu 10.27%\n",
      "iter 370: loss 1.7504, time 35.17ms, mfu 10.30%\n",
      "iter 380: loss 1.7211, time 35.24ms, mfu 10.33%\n",
      "iter 390: loss 1.7369, time 35.11ms, mfu 10.36%\n",
      "iter 400: loss 1.7605, time 35.16ms, mfu 10.38%\n",
      "iter 410: loss 1.7021, time 35.17ms, mfu 10.40%\n",
      "iter 420: loss 1.7109, time 35.30ms, mfu 10.42%\n",
      "iter 430: loss 1.6972, time 35.12ms, mfu 10.44%\n",
      "iter 440: loss 1.6626, time 35.25ms, mfu 10.45%\n",
      "iter 450: loss 1.6483, time 35.26ms, mfu 10.46%\n",
      "iter 460: loss 1.6033, time 35.17ms, mfu 10.48%\n",
      "iter 470: loss 1.6543, time 34.86ms, mfu 10.50%\n",
      "iter 480: loss 1.6198, time 35.10ms, mfu 10.51%\n",
      "iter 490: loss 1.5997, time 35.05ms, mfu 10.52%\n",
      "step 500: train loss 1.5239, val loss 1.7251\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.5985, time 5144.26ms, mfu 9.48%\n",
      "iter 510: loss 1.6143, time 35.34ms, mfu 9.58%\n",
      "iter 520: loss 1.5988, time 35.25ms, mfu 9.68%\n",
      "iter 530: loss 1.5589, time 35.03ms, mfu 9.78%\n",
      "iter 540: loss 1.6180, time 35.32ms, mfu 9.85%\n",
      "iter 550: loss 1.5728, time 35.06ms, mfu 9.93%\n",
      "iter 560: loss 1.5669, time 35.83ms, mfu 9.98%\n",
      "iter 570: loss 1.5674, time 35.03ms, mfu 10.04%\n",
      "iter 580: loss 1.5311, time 35.28ms, mfu 10.10%\n",
      "iter 590: loss 1.4988, time 35.23ms, mfu 10.14%\n",
      "iter 600: loss 1.5175, time 35.05ms, mfu 10.19%\n",
      "iter 610: loss 1.5352, time 35.18ms, mfu 10.23%\n",
      "iter 620: loss 1.5337, time 35.12ms, mfu 10.27%\n",
      "iter 630: loss 1.5122, time 35.35ms, mfu 10.30%\n",
      "iter 640: loss 1.4664, time 35.50ms, mfu 10.32%\n",
      "iter 650: loss 1.5049, time 35.41ms, mfu 10.34%\n",
      "iter 660: loss 1.5093, time 35.08ms, mfu 10.37%\n",
      "iter 670: loss 1.4544, time 35.04ms, mfu 10.39%\n",
      "iter 680: loss 1.5061, time 35.31ms, mfu 10.41%\n",
      "iter 690: loss 1.4613, time 35.23ms, mfu 10.43%\n",
      "iter 700: loss 1.4871, time 35.69ms, mfu 10.43%\n",
      "iter 710: loss 1.4534, time 35.36ms, mfu 10.44%\n",
      "iter 720: loss 1.4443, time 35.48ms, mfu 10.45%\n",
      "iter 730: loss 1.4218, time 35.41ms, mfu 10.45%\n",
      "iter 740: loss 1.4354, time 35.47ms, mfu 10.46%\n",
      "step 750: train loss 1.3639, val loss 1.5935\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 750: loss 1.4223, time 5098.74ms, mfu 9.42%\n",
      "iter 760: loss 1.4417, time 35.19ms, mfu 9.54%\n",
      "iter 770: loss 1.4260, time 35.27ms, mfu 9.64%\n",
      "iter 780: loss 1.4217, time 35.06ms, mfu 9.74%\n",
      "iter 790: loss 1.4242, time 35.41ms, mfu 9.82%\n",
      "iter 800: loss 1.4340, time 35.24ms, mfu 9.89%\n",
      "iter 810: loss 1.3997, time 35.30ms, mfu 9.96%\n",
      "iter 820: loss 1.4144, time 35.64ms, mfu 10.01%\n",
      "iter 830: loss 1.3866, time 35.66ms, mfu 10.05%\n",
      "iter 840: loss 1.4105, time 35.68ms, mfu 10.09%\n",
      "iter 850: loss 1.3974, time 35.46ms, mfu 10.13%\n",
      "iter 860: loss 1.3995, time 35.61ms, mfu 10.17%\n",
      "iter 870: loss 1.4005, time 35.39ms, mfu 10.20%\n",
      "iter 880: loss 1.3780, time 35.13ms, mfu 10.24%\n",
      "iter 890: loss 1.3830, time 35.45ms, mfu 10.27%\n",
      "iter 900: loss 1.3722, time 35.36ms, mfu 10.30%\n",
      "iter 910: loss 1.3274, time 35.30ms, mfu 10.32%\n",
      "iter 920: loss 1.3582, time 35.27ms, mfu 10.35%\n",
      "iter 930: loss 1.3653, time 35.82ms, mfu 10.35%\n",
      "iter 940: loss 1.3443, time 35.49ms, mfu 10.37%\n",
      "iter 950: loss 1.3585, time 35.59ms, mfu 10.38%\n",
      "iter 960: loss 1.3675, time 35.29ms, mfu 10.40%\n",
      "iter 970: loss 1.3597, time 35.40ms, mfu 10.41%\n",
      "iter 980: loss 1.3553, time 35.57ms, mfu 10.42%\n",
      "iter 990: loss 1.3384, time 35.81ms, mfu 10.41%\n",
      "step 1000: train loss 1.2726, val loss 1.5333\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.3382, time 5139.94ms, mfu 9.38%\n",
      "iter 1010: loss 1.3352, time 35.73ms, mfu 9.48%\n",
      "iter 1020: loss 1.3083, time 35.50ms, mfu 9.59%\n",
      "iter 1030: loss 1.3393, time 35.57ms, mfu 9.67%\n",
      "iter 1040: loss 1.3584, time 35.31ms, mfu 9.76%\n",
      "iter 1050: loss 1.2901, time 35.32ms, mfu 9.84%\n",
      "iter 1060: loss 1.3421, time 35.45ms, mfu 9.91%\n",
      "iter 1070: loss 1.3347, time 35.35ms, mfu 9.97%\n",
      "iter 1080: loss 1.3352, time 35.84ms, mfu 10.01%\n",
      "iter 1090: loss 1.3547, time 35.48ms, mfu 10.06%\n",
      "iter 1100: loss 1.3172, time 35.57ms, mfu 10.10%\n",
      "iter 1110: loss 1.3101, time 36.06ms, mfu 10.13%\n",
      "iter 1120: loss 1.3048, time 35.70ms, mfu 10.16%\n",
      "iter 1130: loss 1.2953, time 35.21ms, mfu 10.20%\n",
      "iter 1140: loss 1.3020, time 35.75ms, mfu 10.22%\n",
      "iter 1150: loss 1.3069, time 35.44ms, mfu 10.25%\n",
      "iter 1160: loss 1.3371, time 35.56ms, mfu 10.27%\n",
      "iter 1170: loss 1.3005, time 35.83ms, mfu 10.29%\n",
      "iter 1180: loss 1.3264, time 35.44ms, mfu 10.31%\n",
      "iter 1190: loss 1.2688, time 35.45ms, mfu 10.33%\n",
      "iter 1200: loss 1.2975, time 35.30ms, mfu 10.35%\n",
      "iter 1210: loss 1.2703, time 35.37ms, mfu 10.37%\n",
      "iter 1220: loss 1.3078, time 35.71ms, mfu 10.38%\n",
      "iter 1230: loss 1.2980, time 35.71ms, mfu 10.38%\n",
      "iter 1240: loss 1.3003, time 35.52ms, mfu 10.39%\n",
      "step 1250: train loss 1.2024, val loss 1.4920\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1250: loss 1.2731, time 5185.14ms, mfu 9.36%\n",
      "iter 1260: loss 1.2824, time 35.55ms, mfu 9.47%\n",
      "iter 1270: loss 1.2694, time 35.37ms, mfu 9.58%\n",
      "iter 1280: loss 1.2582, time 35.39ms, mfu 9.67%\n",
      "iter 1290: loss 1.2843, time 35.54ms, mfu 9.76%\n",
      "iter 1300: loss 1.3065, time 35.66ms, mfu 9.82%\n",
      "iter 1310: loss 1.2353, time 35.76ms, mfu 9.88%\n",
      "iter 1320: loss 1.3076, time 35.64ms, mfu 9.94%\n",
      "iter 1330: loss 1.2657, time 35.70ms, mfu 9.99%\n",
      "iter 1340: loss 1.3027, time 35.77ms, mfu 10.03%\n",
      "iter 1350: loss 1.2496, time 35.41ms, mfu 10.08%\n",
      "iter 1360: loss 1.2738, time 35.71ms, mfu 10.12%\n",
      "iter 1370: loss 1.2568, time 35.70ms, mfu 10.15%\n",
      "iter 1380: loss 1.2647, time 35.42ms, mfu 10.19%\n",
      "iter 1390: loss 1.2513, time 35.78ms, mfu 10.21%\n",
      "iter 1400: loss 1.2540, time 35.71ms, mfu 10.23%\n",
      "iter 1410: loss 1.2466, time 35.60ms, mfu 10.26%\n",
      "iter 1420: loss 1.2731, time 35.50ms, mfu 10.28%\n",
      "iter 1430: loss 1.2445, time 35.66ms, mfu 10.30%\n",
      "iter 1440: loss 1.2554, time 35.76ms, mfu 10.31%\n",
      "iter 1450: loss 1.2359, time 35.87ms, mfu 10.32%\n",
      "iter 1460: loss 1.2487, time 35.71ms, mfu 10.33%\n",
      "iter 1470: loss 1.2193, time 35.75ms, mfu 10.34%\n",
      "iter 1480: loss 1.2109, time 35.61ms, mfu 10.35%\n",
      "iter 1490: loss 1.2364, time 35.69ms, mfu 10.36%\n",
      "step 1500: train loss 1.1530, val loss 1.4731\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.1923, time 5146.99ms, mfu 9.33%\n",
      "iter 1510: loss 1.2371, time 35.50ms, mfu 9.45%\n",
      "iter 1520: loss 1.2270, time 35.93ms, mfu 9.54%\n",
      "iter 1530: loss 1.2562, time 35.95ms, mfu 9.62%\n",
      "iter 1540: loss 1.1861, time 35.76ms, mfu 9.70%\n",
      "iter 1550: loss 1.2325, time 36.09ms, mfu 9.76%\n",
      "iter 1560: loss 1.2127, time 35.47ms, mfu 9.84%\n",
      "iter 1570: loss 1.2374, time 35.66ms, mfu 9.90%\n",
      "iter 1580: loss 1.2018, time 35.85ms, mfu 9.95%\n",
      "iter 1590: loss 1.1909, time 35.77ms, mfu 10.00%\n",
      "iter 1600: loss 1.2002, time 35.97ms, mfu 10.03%\n",
      "iter 1610: loss 1.2407, time 35.80ms, mfu 10.07%\n",
      "iter 1620: loss 1.1794, time 36.10ms, mfu 10.09%\n",
      "iter 1630: loss 1.2082, time 35.94ms, mfu 10.12%\n",
      "iter 1640: loss 1.2069, time 35.88ms, mfu 10.15%\n",
      "iter 1650: loss 1.1829, time 35.79ms, mfu 10.17%\n",
      "iter 1660: loss 1.2241, time 36.12ms, mfu 10.19%\n",
      "iter 1670: loss 1.2028, time 36.18ms, mfu 10.20%\n",
      "iter 1680: loss 1.2021, time 36.56ms, mfu 10.20%\n",
      "iter 1690: loss 1.2113, time 35.67ms, mfu 10.22%\n",
      "iter 1700: loss 1.1897, time 35.89ms, mfu 10.24%\n",
      "iter 1710: loss 1.1845, time 35.78ms, mfu 10.26%\n",
      "iter 1720: loss 1.1803, time 35.67ms, mfu 10.28%\n",
      "iter 1730: loss 1.2001, time 35.67ms, mfu 10.29%\n",
      "iter 1740: loss 1.1733, time 36.08ms, mfu 10.30%\n",
      "step 1750: train loss 1.1024, val loss 1.4650\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1750: loss 1.1864, time 5143.51ms, mfu 9.27%\n",
      "iter 1760: loss 1.1848, time 35.62ms, mfu 9.39%\n",
      "iter 1770: loss 1.2051, time 35.83ms, mfu 9.49%\n",
      "iter 1780: loss 1.1984, time 36.16ms, mfu 9.57%\n",
      "iter 1790: loss 1.1971, time 35.82ms, mfu 9.66%\n",
      "iter 1800: loss 1.1861, time 35.86ms, mfu 9.73%\n",
      "iter 1810: loss 1.1625, time 35.80ms, mfu 9.80%\n",
      "iter 1820: loss 1.1727, time 35.81ms, mfu 9.86%\n",
      "iter 1830: loss 1.1680, time 35.67ms, mfu 9.92%\n",
      "iter 1840: loss 1.1616, time 35.81ms, mfu 9.97%\n",
      "iter 1850: loss 1.1645, time 35.92ms, mfu 10.01%\n",
      "iter 1860: loss 1.1816, time 35.72ms, mfu 10.05%\n",
      "iter 1870: loss 1.1408, time 35.67ms, mfu 10.09%\n",
      "iter 1880: loss 1.1699, time 35.70ms, mfu 10.12%\n",
      "iter 1890: loss 1.1813, time 35.85ms, mfu 10.15%\n",
      "iter 1900: loss 1.1314, time 35.94ms, mfu 10.17%\n",
      "iter 1910: loss 1.1708, time 35.78ms, mfu 10.20%\n",
      "iter 1920: loss 1.1747, time 36.02ms, mfu 10.21%\n",
      "iter 1930: loss 1.1462, time 36.04ms, mfu 10.22%\n",
      "iter 1940: loss 1.1299, time 36.04ms, mfu 10.24%\n",
      "iter 1950: loss 1.1463, time 35.97ms, mfu 10.25%\n",
      "iter 1960: loss 1.1585, time 35.91ms, mfu 10.26%\n",
      "iter 1970: loss 1.1556, time 35.86ms, mfu 10.27%\n",
      "iter 1980: loss 1.1521, time 36.09ms, mfu 10.28%\n",
      "iter 1990: loss 1.1551, time 35.93ms, mfu 10.29%\n",
      "step 2000: train loss 1.0559, val loss 1.4791\n",
      "iter 2000: loss 1.1275, time 4545.94ms, mfu 9.27%\n",
      "iter 2010: loss 1.1257, time 36.03ms, mfu 9.38%\n",
      "iter 2020: loss 1.1234, time 36.11ms, mfu 9.47%\n",
      "iter 2030: loss 1.1534, time 36.04ms, mfu 9.56%\n",
      "iter 2040: loss 1.1488, time 36.00ms, mfu 9.64%\n",
      "iter 2050: loss 1.1202, time 36.04ms, mfu 9.71%\n",
      "iter 2060: loss 1.1007, time 36.09ms, mfu 9.77%\n",
      "iter 2070: loss 1.1312, time 35.91ms, mfu 9.83%\n",
      "iter 2080: loss 1.1212, time 36.21ms, mfu 9.88%\n",
      "iter 2090: loss 1.1394, time 36.20ms, mfu 9.92%\n",
      "iter 2100: loss 1.1354, time 35.90ms, mfu 9.96%\n",
      "iter 2110: loss 1.1345, time 35.82ms, mfu 10.01%\n",
      "iter 2120: loss 1.1314, time 36.15ms, mfu 10.04%\n",
      "iter 2130: loss 1.1370, time 35.95ms, mfu 10.07%\n",
      "iter 2140: loss 1.1415, time 36.02ms, mfu 10.10%\n",
      "iter 2150: loss 1.1206, time 35.96ms, mfu 10.12%\n",
      "iter 2160: loss 1.1476, time 36.01ms, mfu 10.15%\n",
      "iter 2170: loss 1.1382, time 35.79ms, mfu 10.17%\n",
      "iter 2180: loss 1.1150, time 36.06ms, mfu 10.19%\n",
      "iter 2190: loss 1.1048, time 36.13ms, mfu 10.20%\n",
      "iter 2200: loss 1.1300, time 36.13ms, mfu 10.21%\n",
      "iter 2210: loss 1.1103, time 35.80ms, mfu 10.23%\n",
      "iter 2220: loss 1.1237, time 36.04ms, mfu 10.24%\n",
      "iter 2230: loss 1.1159, time 36.18ms, mfu 10.25%\n",
      "iter 2240: loss 1.1332, time 35.86ms, mfu 10.26%\n",
      "step 2250: train loss 1.0075, val loss 1.4779\n",
      "iter 2250: loss 1.1044, time 4535.17ms, mfu 9.24%\n",
      "iter 2260: loss 1.1177, time 36.38ms, mfu 9.34%\n",
      "iter 2270: loss 1.1352, time 36.04ms, mfu 9.44%\n",
      "iter 2280: loss 1.1012, time 36.10ms, mfu 9.53%\n",
      "iter 2290: loss 1.1446, time 35.56ms, mfu 9.63%\n",
      "iter 2300: loss 1.1250, time 35.79ms, mfu 9.71%\n",
      "iter 2310: loss 1.0986, time 36.04ms, mfu 9.77%\n",
      "iter 2320: loss 1.1006, time 36.09ms, mfu 9.82%\n",
      "iter 2330: loss 1.0993, time 36.38ms, mfu 9.87%\n",
      "iter 2340: loss 1.1128, time 36.22ms, mfu 9.91%\n",
      "iter 2350: loss 1.1121, time 36.15ms, mfu 9.95%\n",
      "iter 2360: loss 1.1142, time 36.04ms, mfu 9.99%\n",
      "iter 2370: loss 1.0971, time 35.87ms, mfu 10.03%\n",
      "iter 2380: loss 1.0865, time 36.39ms, mfu 10.05%\n",
      "iter 2390: loss 1.0852, time 36.07ms, mfu 10.08%\n",
      "iter 2400: loss 1.0778, time 35.94ms, mfu 10.11%\n",
      "iter 2410: loss 1.0761, time 36.00ms, mfu 10.13%\n",
      "iter 2420: loss 1.0842, time 35.79ms, mfu 10.16%\n",
      "iter 2430: loss 1.0552, time 35.83ms, mfu 10.18%\n",
      "iter 2440: loss 1.0626, time 36.07ms, mfu 10.20%\n",
      "iter 2450: loss 1.0773, time 35.85ms, mfu 10.22%\n",
      "iter 2460: loss 1.0899, time 36.20ms, mfu 10.22%\n",
      "iter 2470: loss 1.0897, time 36.09ms, mfu 10.23%\n",
      "iter 2480: loss 1.0887, time 36.04ms, mfu 10.25%\n",
      "iter 2490: loss 1.0553, time 35.99ms, mfu 10.26%\n",
      "step 2500: train loss 0.9634, val loss 1.4923\n",
      "iter 2500: loss 1.0872, time 4550.77ms, mfu 9.24%\n",
      "iter 2510: loss 1.0723, time 36.05ms, mfu 9.35%\n",
      "iter 2520: loss 1.0546, time 36.28ms, mfu 9.44%\n",
      "iter 2530: loss 1.0521, time 36.41ms, mfu 9.52%\n",
      "iter 2540: loss 1.0566, time 36.35ms, mfu 9.59%\n",
      "iter 2550: loss 1.0736, time 35.97ms, mfu 9.67%\n",
      "iter 2560: loss 1.0591, time 36.37ms, mfu 9.73%\n",
      "iter 2570: loss 1.0848, time 36.41ms, mfu 9.78%\n",
      "iter 2580: loss 1.0780, time 36.38ms, mfu 9.82%\n",
      "iter 2590: loss 1.0650, time 36.25ms, mfu 9.87%\n",
      "iter 2600: loss 1.0641, time 36.48ms, mfu 9.90%\n",
      "iter 2610: loss 1.0474, time 36.39ms, mfu 9.94%\n",
      "iter 2620: loss 1.0423, time 35.94ms, mfu 9.98%\n",
      "iter 2630: loss 1.0275, time 36.43ms, mfu 10.01%\n",
      "iter 2640: loss 1.0450, time 36.35ms, mfu 10.03%\n",
      "iter 2650: loss 1.0694, time 36.52ms, mfu 10.05%\n",
      "iter 2660: loss 1.0517, time 36.59ms, mfu 10.06%\n",
      "iter 2670: loss 1.0220, time 36.00ms, mfu 10.09%\n",
      "iter 2680: loss 1.0571, time 36.13ms, mfu 10.11%\n",
      "iter 2690: loss 1.0608, time 35.91ms, mfu 10.14%\n",
      "iter 2700: loss 1.0364, time 36.48ms, mfu 10.15%\n",
      "iter 2710: loss 1.0530, time 36.40ms, mfu 10.16%\n",
      "iter 2720: loss 1.0484, time 36.56ms, mfu 10.16%\n",
      "iter 2730: loss 1.0673, time 35.92ms, mfu 10.18%\n",
      "iter 2740: loss 1.0283, time 36.09ms, mfu 10.20%\n",
      "step 2750: train loss 0.9160, val loss 1.5087\n",
      "iter 2750: loss 1.0370, time 4541.03ms, mfu 9.18%\n",
      "iter 2760: loss 1.0367, time 36.24ms, mfu 9.29%\n",
      "iter 2770: loss 1.0296, time 35.84ms, mfu 9.40%\n",
      "iter 2780: loss 1.0282, time 36.26ms, mfu 9.49%\n",
      "iter 2790: loss 1.0474, time 35.91ms, mfu 9.58%\n",
      "iter 2800: loss 1.0192, time 35.99ms, mfu 9.66%\n",
      "iter 2810: loss 1.0402, time 36.19ms, mfu 9.72%\n",
      "iter 2820: loss 1.0244, time 36.84ms, mfu 9.76%\n",
      "iter 2830: loss 1.0409, time 36.25ms, mfu 9.81%\n",
      "iter 2840: loss 0.9978, time 36.11ms, mfu 9.86%\n",
      "iter 2850: loss 1.0358, time 36.17ms, mfu 9.91%\n",
      "iter 2860: loss 1.0289, time 36.34ms, mfu 9.94%\n",
      "iter 2870: loss 1.0108, time 36.09ms, mfu 9.98%\n",
      "iter 2880: loss 1.0357, time 36.31ms, mfu 10.01%\n",
      "iter 2890: loss 1.0066, time 36.20ms, mfu 10.04%\n",
      "iter 2900: loss 0.9995, time 36.18ms, mfu 10.06%\n",
      "iter 2910: loss 1.0348, time 36.32ms, mfu 10.08%\n",
      "iter 2920: loss 1.0254, time 36.11ms, mfu 10.11%\n",
      "iter 2930: loss 0.9957, time 36.44ms, mfu 10.12%\n",
      "iter 2940: loss 0.9992, time 36.08ms, mfu 10.14%\n",
      "iter 2950: loss 1.0353, time 36.03ms, mfu 10.16%\n",
      "iter 2960: loss 1.0002, time 36.11ms, mfu 10.18%\n",
      "iter 2970: loss 0.9943, time 35.94ms, mfu 10.19%\n",
      "iter 2980: loss 1.0045, time 36.42ms, mfu 10.20%\n",
      "iter 2990: loss 0.9920, time 36.12ms, mfu 10.21%\n",
      "step 3000: train loss 0.8704, val loss 1.5133\n",
      "iter 3000: loss 0.9849, time 4535.53ms, mfu 9.20%\n",
      "iter 3010: loss 1.0001, time 36.04ms, mfu 9.31%\n",
      "iter 3020: loss 1.0045, time 36.35ms, mfu 9.41%\n",
      "iter 3030: loss 1.0050, time 36.33ms, mfu 9.49%\n",
      "iter 3040: loss 1.0268, time 36.38ms, mfu 9.57%\n",
      "iter 3050: loss 0.9872, time 36.42ms, mfu 9.63%\n",
      "iter 3060: loss 0.9965, time 36.01ms, mfu 9.70%\n",
      "iter 3070: loss 1.0250, time 36.11ms, mfu 9.77%\n",
      "iter 3080: loss 1.0070, time 36.19ms, mfu 9.82%\n",
      "iter 3090: loss 0.9885, time 36.00ms, mfu 9.87%\n",
      "iter 3100: loss 0.9948, time 36.20ms, mfu 9.91%\n",
      "iter 3110: loss 0.9766, time 35.97ms, mfu 9.96%\n",
      "iter 3120: loss 1.0045, time 36.42ms, mfu 9.99%\n",
      "iter 3130: loss 0.9846, time 36.21ms, mfu 10.02%\n",
      "iter 3140: loss 0.9778, time 36.46ms, mfu 10.04%\n",
      "iter 3150: loss 0.9927, time 36.38ms, mfu 10.06%\n",
      "iter 3160: loss 1.0138, time 36.58ms, mfu 10.07%\n",
      "iter 3170: loss 0.9601, time 36.20ms, mfu 10.09%\n",
      "iter 3180: loss 0.9762, time 36.45ms, mfu 10.11%\n",
      "iter 3190: loss 0.9941, time 36.07ms, mfu 10.13%\n",
      "iter 3200: loss 0.9727, time 36.11ms, mfu 10.15%\n",
      "iter 3210: loss 0.9701, time 36.33ms, mfu 10.16%\n",
      "iter 3220: loss 0.9611, time 36.09ms, mfu 10.17%\n",
      "iter 3230: loss 0.9559, time 36.25ms, mfu 10.19%\n",
      "iter 3240: loss 0.9645, time 36.26ms, mfu 10.19%\n",
      "step 3250: train loss 0.8267, val loss 1.5546\n",
      "iter 3250: loss 0.9718, time 4531.25ms, mfu 9.18%\n",
      "iter 3260: loss 0.9745, time 36.28ms, mfu 9.29%\n",
      "iter 3270: loss 0.9788, time 36.14ms, mfu 9.39%\n",
      "iter 3280: loss 0.9574, time 36.18ms, mfu 9.48%\n",
      "iter 3290: loss 0.9455, time 36.54ms, mfu 9.56%\n",
      "iter 3300: loss 0.9446, time 36.08ms, mfu 9.63%\n",
      "iter 3310: loss 0.9541, time 36.32ms, mfu 9.70%\n",
      "iter 3320: loss 0.9658, time 35.95ms, mfu 9.76%\n",
      "iter 3330: loss 0.9565, time 36.42ms, mfu 9.81%\n",
      "iter 3340: loss 0.9514, time 36.45ms, mfu 9.85%\n",
      "iter 3350: loss 0.9602, time 36.40ms, mfu 9.89%\n",
      "iter 3360: loss 0.9358, time 36.33ms, mfu 9.93%\n",
      "iter 3370: loss 0.9631, time 35.99ms, mfu 9.97%\n",
      "iter 3380: loss 0.9466, time 36.13ms, mfu 10.00%\n",
      "iter 3390: loss 0.9588, time 36.13ms, mfu 10.03%\n",
      "iter 3400: loss 0.9555, time 36.63ms, mfu 10.05%\n",
      "iter 3410: loss 0.9520, time 36.49ms, mfu 10.06%\n",
      "iter 3420: loss 0.9524, time 36.52ms, mfu 10.08%\n",
      "iter 3430: loss 0.9488, time 36.36ms, mfu 10.10%\n",
      "iter 3440: loss 0.9803, time 36.16ms, mfu 10.12%\n",
      "iter 3450: loss 0.9548, time 36.37ms, mfu 10.13%\n",
      "iter 3460: loss 0.9461, time 36.20ms, mfu 10.15%\n",
      "iter 3470: loss 0.9418, time 36.37ms, mfu 10.16%\n",
      "iter 3480: loss 0.9566, time 36.18ms, mfu 10.17%\n",
      "iter 3490: loss 0.9225, time 36.45ms, mfu 10.17%\n",
      "step 3500: train loss 0.7859, val loss 1.5710\n",
      "iter 3500: loss 0.9173, time 4504.54ms, mfu 9.17%\n",
      "iter 3510: loss 0.9198, time 35.97ms, mfu 9.29%\n",
      "iter 3520: loss 0.9257, time 36.38ms, mfu 9.38%\n",
      "iter 3530: loss 0.9514, time 36.15ms, mfu 9.47%\n",
      "iter 3540: loss 0.9349, time 36.57ms, mfu 9.55%\n",
      "iter 3550: loss 0.9332, time 36.16ms, mfu 9.62%\n",
      "iter 3560: loss 0.9584, time 36.36ms, mfu 9.68%\n",
      "iter 3570: loss 0.9348, time 36.09ms, mfu 9.75%\n",
      "iter 3580: loss 0.9332, time 36.14ms, mfu 9.80%\n",
      "iter 3590: loss 0.9326, time 36.01ms, mfu 9.86%\n",
      "iter 3600: loss 0.9218, time 36.27ms, mfu 9.90%\n",
      "iter 3610: loss 0.9164, time 36.23ms, mfu 9.94%\n",
      "iter 3620: loss 0.9218, time 36.20ms, mfu 9.97%\n",
      "iter 3630: loss 0.9293, time 36.74ms, mfu 9.99%\n",
      "iter 3640: loss 0.9264, time 36.25ms, mfu 10.02%\n",
      "iter 3650: loss 0.9237, time 36.05ms, mfu 10.05%\n",
      "iter 3660: loss 0.9352, time 36.38ms, mfu 10.07%\n",
      "iter 3670: loss 0.9365, time 36.31ms, mfu 10.09%\n",
      "iter 3680: loss 0.9121, time 36.34ms, mfu 10.11%\n",
      "iter 3690: loss 0.9489, time 36.32ms, mfu 10.12%\n",
      "iter 3700: loss 0.8769, time 36.63ms, mfu 10.13%\n",
      "iter 3710: loss 0.8857, time 36.10ms, mfu 10.15%\n",
      "iter 3720: loss 0.9054, time 36.31ms, mfu 10.16%\n",
      "iter 3730: loss 0.9047, time 36.05ms, mfu 10.18%\n",
      "iter 3740: loss 0.9086, time 36.29ms, mfu 10.18%\n",
      "step 3750: train loss 0.7487, val loss 1.5945\n",
      "iter 3750: loss 0.8968, time 4529.73ms, mfu 9.17%\n",
      "iter 3760: loss 0.9490, time 36.19ms, mfu 9.29%\n",
      "iter 3770: loss 0.9369, time 36.07ms, mfu 9.39%\n",
      "iter 3780: loss 0.9295, time 36.09ms, mfu 9.48%\n",
      "iter 3790: loss 0.9043, time 36.38ms, mfu 9.56%\n",
      "iter 3800: loss 0.9174, time 36.38ms, mfu 9.63%\n",
      "iter 3810: loss 0.9278, time 36.54ms, mfu 9.69%\n",
      "iter 3820: loss 0.8949, time 36.14ms, mfu 9.75%\n",
      "iter 3830: loss 0.9004, time 36.42ms, mfu 9.80%\n",
      "iter 3840: loss 0.9048, time 36.51ms, mfu 9.84%\n",
      "iter 3850: loss 0.8872, time 36.75ms, mfu 9.87%\n",
      "iter 3860: loss 0.8810, time 36.24ms, mfu 9.91%\n",
      "iter 3870: loss 0.9045, time 36.18ms, mfu 9.95%\n",
      "iter 3880: loss 0.8940, time 36.27ms, mfu 9.98%\n",
      "iter 3890: loss 0.8943, time 36.26ms, mfu 10.01%\n",
      "iter 3900: loss 0.9063, time 36.90ms, mfu 10.02%\n",
      "iter 3910: loss 0.8993, time 36.49ms, mfu 10.04%\n",
      "iter 3920: loss 0.8754, time 37.63ms, mfu 10.02%\n",
      "iter 3930: loss 0.8989, time 36.25ms, mfu 10.05%\n",
      "iter 3940: loss 0.8806, time 36.33ms, mfu 10.07%\n",
      "iter 3950: loss 0.8806, time 36.63ms, mfu 10.08%\n",
      "iter 3960: loss 0.9077, time 36.36ms, mfu 10.10%\n",
      "iter 3970: loss 0.8905, time 36.27ms, mfu 10.12%\n",
      "iter 3980: loss 0.9112, time 36.03ms, mfu 10.14%\n",
      "iter 3990: loss 0.8829, time 36.50ms, mfu 10.14%\n",
      "step 4000: train loss 0.7157, val loss 1.6105\n",
      "iter 4000: loss 0.8617, time 4530.12ms, mfu 9.14%\n",
      "iter 4010: loss 0.8832, time 36.17ms, mfu 9.25%\n",
      "iter 4020: loss 0.8978, time 36.38ms, mfu 9.35%\n",
      "iter 4030: loss 0.8854, time 36.26ms, mfu 9.45%\n",
      "iter 4040: loss 0.8820, time 35.98ms, mfu 9.54%\n",
      "iter 4050: loss 0.8785, time 35.89ms, mfu 9.62%\n",
      "iter 4060: loss 0.8650, time 36.47ms, mfu 9.68%\n",
      "iter 4070: loss 0.8626, time 36.64ms, mfu 9.73%\n",
      "iter 4080: loss 0.8821, time 36.02ms, mfu 9.79%\n",
      "iter 4090: loss 0.8523, time 36.74ms, mfu 9.83%\n",
      "iter 4100: loss 0.9066, time 36.58ms, mfu 9.86%\n",
      "iter 4110: loss 0.8802, time 36.11ms, mfu 9.91%\n",
      "iter 4120: loss 0.8832, time 36.60ms, mfu 9.94%\n",
      "iter 4130: loss 0.8701, time 36.19ms, mfu 9.97%\n",
      "iter 4140: loss 0.8788, time 36.48ms, mfu 10.00%\n",
      "iter 4150: loss 0.8865, time 36.02ms, mfu 10.03%\n",
      "iter 4160: loss 0.8545, time 36.48ms, mfu 10.05%\n",
      "iter 4170: loss 0.8810, time 36.33ms, mfu 10.07%\n",
      "iter 4180: loss 0.8692, time 35.98ms, mfu 10.10%\n",
      "iter 4190: loss 0.8730, time 36.28ms, mfu 10.12%\n",
      "iter 4200: loss 0.8716, time 36.25ms, mfu 10.13%\n",
      "iter 4210: loss 0.8745, time 36.39ms, mfu 10.14%\n",
      "iter 4220: loss 0.8667, time 36.23ms, mfu 10.16%\n",
      "iter 4230: loss 0.8896, time 36.48ms, mfu 10.16%\n",
      "iter 4240: loss 0.8759, time 36.17ms, mfu 10.18%\n",
      "step 4250: train loss 0.6835, val loss 1.6388\n",
      "iter 4250: loss 0.8744, time 4531.91ms, mfu 9.17%\n",
      "iter 4260: loss 0.8690, time 36.17ms, mfu 9.28%\n",
      "iter 4270: loss 0.8645, time 36.17ms, mfu 9.38%\n",
      "iter 4280: loss 0.8694, time 36.09ms, mfu 9.48%\n",
      "iter 4290: loss 0.8385, time 36.34ms, mfu 9.55%\n",
      "iter 4300: loss 0.8357, time 35.93ms, mfu 9.64%\n",
      "iter 4310: loss 0.8526, time 36.40ms, mfu 9.70%\n",
      "iter 4320: loss 0.8442, time 36.66ms, mfu 9.74%\n",
      "iter 4330: loss 0.8714, time 36.41ms, mfu 9.79%\n",
      "iter 4340: loss 0.8364, time 36.40ms, mfu 9.84%\n",
      "iter 4350: loss 0.8529, time 36.37ms, mfu 9.88%\n",
      "iter 4360: loss 0.8622, time 36.01ms, mfu 9.92%\n",
      "iter 4370: loss 0.8654, time 36.33ms, mfu 9.96%\n",
      "iter 4380: loss 0.8459, time 36.44ms, mfu 9.98%\n",
      "iter 4390: loss 0.8644, time 36.55ms, mfu 10.01%\n",
      "iter 4400: loss 0.8547, time 36.64ms, mfu 10.02%\n",
      "iter 4410: loss 0.8656, time 36.22ms, mfu 10.05%\n",
      "iter 4420: loss 0.8624, time 36.52ms, mfu 10.06%\n",
      "iter 4430: loss 0.8529, time 36.06ms, mfu 10.09%\n",
      "iter 4440: loss 0.8496, time 36.09ms, mfu 10.11%\n",
      "iter 4450: loss 0.8554, time 36.54ms, mfu 10.12%\n",
      "iter 4460: loss 0.8357, time 36.18ms, mfu 10.14%\n",
      "iter 4470: loss 0.8492, time 36.38ms, mfu 10.15%\n",
      "iter 4480: loss 0.8397, time 36.38ms, mfu 10.16%\n",
      "iter 4490: loss 0.8522, time 36.08ms, mfu 10.18%\n",
      "step 4500: train loss 0.6594, val loss 1.6557\n",
      "iter 4500: loss 0.8643, time 4537.33ms, mfu 9.17%\n",
      "iter 4510: loss 0.8602, time 36.56ms, mfu 9.27%\n",
      "iter 4520: loss 0.8465, time 36.38ms, mfu 9.37%\n",
      "iter 4530: loss 0.8583, time 36.44ms, mfu 9.45%\n",
      "iter 4540: loss 0.8543, time 36.36ms, mfu 9.53%\n",
      "iter 4550: loss 0.8762, time 36.30ms, mfu 9.61%\n",
      "iter 4560: loss 0.8437, time 36.49ms, mfu 9.67%\n",
      "iter 4570: loss 0.8577, time 36.37ms, mfu 9.72%\n",
      "iter 4580: loss 0.8572, time 36.19ms, mfu 9.78%\n",
      "iter 4590: loss 0.8601, time 36.66ms, mfu 9.82%\n",
      "iter 4600: loss 0.8293, time 36.59ms, mfu 9.86%\n",
      "iter 4610: loss 0.8716, time 36.10ms, mfu 9.90%\n",
      "iter 4620: loss 0.8455, time 36.24ms, mfu 9.94%\n",
      "iter 4630: loss 0.8332, time 36.37ms, mfu 9.97%\n",
      "iter 4640: loss 0.8519, time 36.18ms, mfu 10.00%\n",
      "iter 4650: loss 0.8611, time 35.98ms, mfu 10.04%\n",
      "iter 4660: loss 0.8503, time 36.29ms, mfu 10.06%\n",
      "iter 4670: loss 0.8533, time 36.15ms, mfu 10.09%\n",
      "iter 4680: loss 0.8546, time 36.54ms, mfu 10.10%\n",
      "iter 4690: loss 0.8515, time 36.22ms, mfu 10.12%\n",
      "iter 4700: loss 0.8276, time 36.26ms, mfu 10.13%\n",
      "iter 4710: loss 0.7987, time 36.06ms, mfu 10.15%\n",
      "iter 4720: loss 0.8394, time 36.40ms, mfu 10.16%\n",
      "iter 4730: loss 0.8334, time 36.07ms, mfu 10.18%\n",
      "iter 4740: loss 0.8309, time 36.39ms, mfu 10.18%\n",
      "step 4750: train loss 0.6429, val loss 1.6762\n",
      "iter 4750: loss 0.8088, time 4544.03ms, mfu 9.17%\n",
      "iter 4760: loss 0.8261, time 36.25ms, mfu 9.28%\n",
      "iter 4770: loss 0.8074, time 36.11ms, mfu 9.39%\n",
      "iter 4780: loss 0.8161, time 36.14ms, mfu 9.48%\n",
      "iter 4790: loss 0.8491, time 36.33ms, mfu 9.56%\n",
      "iter 4800: loss 0.8260, time 36.25ms, mfu 9.63%\n",
      "iter 4810: loss 0.8545, time 36.44ms, mfu 9.69%\n",
      "iter 4820: loss 0.8228, time 36.31ms, mfu 9.75%\n",
      "iter 4830: loss 0.8287, time 36.08ms, mfu 9.81%\n",
      "iter 4840: loss 0.8372, time 36.28ms, mfu 9.85%\n",
      "iter 4850: loss 0.8291, time 36.02ms, mfu 9.90%\n",
      "iter 4860: loss 0.8194, time 36.00ms, mfu 9.95%\n",
      "iter 4870: loss 0.8039, time 36.43ms, mfu 9.97%\n",
      "iter 4880: loss 0.8310, time 36.30ms, mfu 10.00%\n",
      "iter 4890: loss 0.8217, time 36.60ms, mfu 10.02%\n",
      "iter 4900: loss 0.8077, time 36.12ms, mfu 10.05%\n",
      "iter 4910: loss 0.8331, time 36.36ms, mfu 10.07%\n",
      "iter 4920: loss 0.8261, time 36.40ms, mfu 10.09%\n",
      "iter 4930: loss 0.8106, time 36.03ms, mfu 10.11%\n",
      "iter 4940: loss 0.8003, time 36.19ms, mfu 10.13%\n",
      "iter 4950: loss 0.8335, time 36.09ms, mfu 10.15%\n",
      "iter 4960: loss 0.8290, time 37.11ms, mfu 10.14%\n",
      "iter 4970: loss 0.7888, time 36.00ms, mfu 10.16%\n",
      "iter 4980: loss 0.8011, time 36.44ms, mfu 10.17%\n",
      "iter 4990: loss 0.8384, time 36.24ms, mfu 10.18%\n",
      "step 5000: train loss 0.6284, val loss 1.6904\n",
      "iter 5000: loss 0.8200, time 4552.34ms, mfu 9.17%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_shakespeare_char.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdbaf6c-f061-43e1-a53b-01770a3bd592",
   "metadata": {},
   "source": [
    "### Sample/Inference\n",
    "\n",
    "Once the training finishes we can sample from the best model by pointing the sampling script at this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6696065-275f-4c2a-b965-fe6489510731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "number of parameters: 10.65M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "\n",
      "ANGELO:\n",
      "And cowards it be straighted but our hands.\n",
      "3 KING EDWARD IV:\n",
      "What, uncle? therefore thou enjoy'st us not the more\n",
      "When we have finity to my foul ancient reels,\n",
      "I hear thee in our soul. I know not.\n",
      "\n",
      "Gaoler:\n",
      "I am a world-husband with a cheeking here,\n",
      "To seek thy foot so flesh in the demand.\n",
      "\n",
      "KING EDWARD IV:\n",
      "But we are done with us in his soul:\n",
      "The rest was my brother son; which must with all of woes\n",
      "Every gross corruptuous strength:\n",
      "Are all comes to be in courtesy?\n",
      "\n",
      "Son:\n",
      "It is a man are \n",
      "---------------\n",
      "\n",
      "Menenius, and graves your gatesmen have more.\n",
      "\n",
      "AUFIDIUS:\n",
      "The matter?\n",
      "\n",
      "CORIOLANUS:\n",
      "There is my wife.\n",
      "\n",
      "MENENIUS:\n",
      "I have look'd it bounds in the season.\n",
      "\n",
      "BRUTUS:\n",
      "We had as committed to come too so undoubtled as the\n",
      "servant: I have sent the violence.\n",
      "\n",
      "COMINIUS:\n",
      "Nor bear the world that all to the very whole than\n",
      "I have consul, come I see thee in the people's new that man\n",
      "But to this offence?\n",
      "\n",
      "MENENIUS:\n",
      "Commend thee thou as the custom, whose knaven sound\n",
      "For those twalls that would have used thee and \n",
      "---------------\n",
      "\n",
      "Men take me with all the two compass.\n",
      "Most grief, here comes; I am not well surer\n",
      "But we with mine eyes; and till you he were took\n",
      "And tell me with me what you shame to take the crown.\n",
      "\n",
      "LADY CAPULET:\n",
      "And let me see the world of winningers.\n",
      "\n",
      "Nurse:\n",
      "I did to see you for Hereford's land;\n",
      "And give me the earth o' the soul,\n",
      "Make him a little substant as stand,\n",
      "Albass doth him to be a harsh.\n",
      "\n",
      "Nurse:\n",
      "What but news? what news about of me?\n",
      "Which will you take me speak.\n",
      "\n",
      "Nurse:\n",
      "He is your son and makes hi\n",
      "---------------\n",
      "\n",
      "\n",
      "First Murderer:\n",
      "My lord, when I saw; for what a woman can rest!\n",
      "\n",
      "CLIFFORD:\n",
      "Have done, my lord, we will draw the dead of his\n",
      "fortune.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Through me with Gaunt, give me before my heart.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Why, well, no bloody brother, brother I have more company.\n",
      "\n",
      "RICHARD:\n",
      "Not a hardy with sweet fair a slander,\n",
      "Where you have dislike he heard my drum, and wakes my windows.\n",
      "\n",
      "BUCKINGHAM:\n",
      "What now? what same a brather? what says that are not brief?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Give me the villain and u\n",
      "---------------\n",
      "\n",
      "BUCKINGHAM:\n",
      "Then, his lords will stay to grow.\n",
      "Gracious life, look you, Aumerle, with this changed\n",
      "That brook a world whole enough I weight your stead\n",
      "To offend you in good country's tings as you at my husband\n",
      "To leave his mother, never bear me,\n",
      "As he is some story, and then untimely.\n",
      "\n",
      "CORIOLANUS:\n",
      "Had he had we with no more upon this deed.\n",
      "\n",
      "CORIOLANUS:\n",
      "Good good my lord, go see him to your company.\n",
      "\n",
      "AUFIDIUS:\n",
      "Anon of that visit of his pawn:\n",
      "If it be would not well less not tell him; he was not b\n",
      "---------------\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "Having he done, and though he makest an image\n",
      "Have answer'd in his person? I hear, he would for him\n",
      "By all the people which begg'd with his rid,\n",
      "Who shall not be the doubted burning one:\n",
      "But if no other wisely the house of Marcius king\n",
      "From his voices could call me alone,\n",
      "So doubt not to path our present gravernments,\n",
      "For he shall in the air of our eyes of God,\n",
      "Which will we what will to prove your deserving shame.\n",
      "Were your voices you by the grave of presently?\n",
      "\n",
      "MERCUTIO:\n",
      "We call'd t\n",
      "---------------\n",
      "\n",
      "Shall be your holy rather gracious shame,\n",
      "You shall shed your son of such as the sweetest\n",
      "As whose herate will false: all whose fourself,\n",
      "You do as it in yourself and my own liege,\n",
      "And I would not have found you to greet my wife,\n",
      "Your observices when you changed on part of my side,\n",
      "Nor the world speak of myself.\n",
      "\n",
      "GLOUCESTER:\n",
      "Not to see, we are father too.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Never seem in a puny too.\n",
      "\n",
      "LUCIO:\n",
      "You have satest well.\n",
      "\n",
      "LUCIO:\n",
      "'Tis a good transport a man: to instruct your hands.\n",
      "\n",
      "DUKE VI\n",
      "---------------\n",
      "\n",
      "lady, who doth sir?\n",
      "\n",
      "BAPTISTA:\n",
      "I mean to heaven, and I'll be not gone.\n",
      "\n",
      "CAPULET:\n",
      "None, good lord, good and girl.\n",
      "\n",
      "BALTHASAR:\n",
      "A cup, this best that is a letter kneel.\n",
      "Why, what of grief?\n",
      "\n",
      "CAMILLO:\n",
      "I have forgot\n",
      "My lipth of your senses unto King Richmond in ears\n",
      "And stand tender to the stirr'd of your steed,\n",
      "Your highness! for comes he hath sent me about your hands.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "If you do it do demand, you speak upon me,\n",
      "Petition more of than a king with of my such men\n",
      "Than your suits to be\n",
      "---------------\n",
      "\n",
      "\n",
      "LEONTES:\n",
      "The rest pass, you have and proceeded: you're it is\n",
      "content of very servant pettion so sweet.\n",
      "\n",
      "PAULINA:\n",
      "I think your flies of choicers of me; I\n",
      "love you, your mother, and put your to me a lord.\n",
      "\n",
      "LEONTES:\n",
      "I must deny you, I wrong you.\n",
      "\n",
      "ANGELO:\n",
      "Sir, I am going into the oil and\n",
      "For your subject dangerous souls a wild deputy:\n",
      "I do not stay that you were seen in his imposter,\n",
      "And with well-as your sons with one of your brothers.\n",
      "\n",
      "ANGELO:\n",
      "Say, sure you that the reason of men,\n",
      "It is a better \n",
      "---------------\n",
      "\n",
      "Her charge is a dream:\n",
      "A priest day, most death, and how I lived!\n",
      "\n",
      "QUEEN MARGARET:\n",
      "With sheeping Bolingbroke cannot so say but by night.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why, even a woman's fear, or work with my law,\n",
      "That with shame and for stone of such law modesty\n",
      "Gove him like off our plant-disposed wings!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Then we for her shed in removes his foe.\n",
      "\n",
      "KING RICHARD III:\n",
      "Brother, honest thou this laught have been sword.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Why, is the common of it?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I would not re\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef341c7-35a5-41d1-9f93-c1bacde3ce6f",
   "metadata": {},
   "source": [
    "### Fine-tune\n",
    "\n",
    "Get data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89cfe9f9-e5ae-4d66-ace0-df158b6e8af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nanoGPT/data/shakespeare\n"
     ]
    }
   ],
   "source": [
    "cd data/shakespeare/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9447aea1-a74c-4c79-a013-58195b14776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "!python prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9e9ad02-3dbf-43a8-9a4e-99f6e6d3d2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ea1f3-5616-44d1-b60c-ec679f234383",
   "metadata": {},
   "source": [
    "Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. \n",
    "\n",
    "Fine-tune a gpt2 model. If you're running out of memory, the process may stop silently, try decreasing the model size (they are {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}). Change in config/finetune_shakespeare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59f7107-a40e-4ff3-9994-530c4cbf890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/finetune_shakespeare.py:\n",
      "import time\n",
      "\n",
      "out_dir = 'out-shakespeare'\n",
      "eval_interval = 5\n",
      "eval_iters = 40\n",
      "wandb_log = False # feel free to turn on\n",
      "wandb_project = 'shakespeare'\n",
      "wandb_run_name = 'ft-' + str(time.time())\n",
      "\n",
      "dataset = 'shakespeare'\n",
      "init_from = 'gpt2-medium' # this is the largest GPT-2 model\n",
      "\n",
      "# only save checkpoints if the validation loss improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "# the number of examples per iter:\n",
      "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
      "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
      "batch_size = 1\n",
      "gradient_accumulation_steps = 32\n",
      "max_iters = 20\n",
      "\n",
      "# finetune at constant LR\n",
      "learning_rate = 3e-5\n",
      "decay_lr = False\n",
      "\n",
      "tokens per iteration will be: 32,768\n",
      "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
      "[2023-08-25 06:30:22,719] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "loading weights from pretrained gpt: gpt2-medium\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 353.77M\n",
      "Downloading model.safetensors: 100%|███████| 1.52G/1.52G [01:30<00:00, 16.9MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████| 124/124 [00:00<00:00, 18.5kB/s]\n",
      "num decayed parameter tensors: 98, with 354,501,632 parameters\n",
      "num non-decayed parameter tensors: 194, with 321,536 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 3.5442, val loss 3.3999\n",
      "iter 0: loss 3.3380, time 51314.72ms, mfu -100.00%\n",
      "iter 1: loss 2.9308, time 3516.80ms, mfu -100.00%\n",
      "iter 2: loss 3.7984, time 3526.72ms, mfu -100.00%\n",
      "iter 3: loss 3.3298, time 3525.95ms, mfu -100.00%\n",
      "iter 4: loss 3.2597, time 3524.82ms, mfu -100.00%\n",
      "step 5: train loss 3.3681, val loss 3.1421\n",
      "saving checkpoint to out-shakespeare\n",
      "iter 5: loss 3.6512, time 12313.84ms, mfu 2.07%\n",
      "iter 6: loss 2.8223, time 3520.38ms, mfu 2.58%\n",
      "iter 7: loss 3.4223, time 3520.08ms, mfu 3.05%\n",
      "iter 8: loss 3.5986, time 3519.64ms, mfu 3.47%\n",
      "iter 9: loss 3.8399, time 3521.61ms, mfu 3.84%\n",
      "step 10: train loss 3.1842, val loss 3.0693\n",
      "saving checkpoint to out-shakespeare\n",
      "iter 10: loss 3.0684, time 50378.93ms, mfu 3.51%\n",
      "iter 11: loss 2.6657, time 3526.71ms, mfu 3.88%\n",
      "iter 12: loss 3.1984, time 3525.26ms, mfu 4.22%\n",
      "iter 13: loss 3.1357, time 3524.47ms, mfu 4.52%\n",
      "iter 14: loss 3.3709, time 3525.21ms, mfu 4.79%\n",
      "step 15: train loss 3.2156, val loss 3.0918\n",
      "iter 15: loss 3.0329, time 5362.16ms, mfu 4.78%\n",
      "iter 16: loss 2.9461, time 3521.49ms, mfu 5.03%\n",
      "iter 17: loss 3.2065, time 3522.36ms, mfu 5.25%\n",
      "iter 18: loss 2.9303, time 3525.65ms, mfu 5.45%\n",
      "iter 19: loss 3.3326, time 3525.07ms, mfu 5.62%\n",
      "step 20: train loss 3.1659, val loss 3.0330\n",
      "saving checkpoint to out-shakespeare\n",
      "iter 20: loss 3.3020, time 49371.47ms, mfu 5.11%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/finetune_shakespeare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad16f1da-77ea-4c3f-8266-3fc00b431edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare\n",
      "number of parameters: 353.77M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "This time, the young man came to the door of the house and asked for bread; and he, receiving it, went to work with his hands to make it.\n",
      "\n",
      "And he struck off a piece of bread, and made into four equal portions, and put it before the king; and he, receiving it, set it upon the table; and he, having received the two pieces of bread, received the other.\n",
      "\n",
      "And the king was greatly satisfied; and said:\n",
      "'You shall have three pieces of bread, for I have two pieces of bread.\n",
      "\n",
      "And in the first piece of bread you shall have an oat for your neighbour's daughter; in the second piece of bread ye shall have three white barley-cake cakes.\n",
      "\n",
      "And in the second piece of bread you shall have one white bread cake; and in the third, a piece of taffy.\n",
      "\n",
      "And in the fourth piece of bread you shall have two pieces of white bread cakes; and in the fifth, two pieces of barley-cake cakes; and in the sixth, a piece of flax.\n",
      "\n",
      "And in the sixth piece of white bread cakes ye shall have a piece of bread, and in the seventh a piece of barley-cake cakes.\n",
      "\n",
      "And in the eighth piece of white bread cakes ye shall have a piece of white bread cakes; and in the ninth, one piece of flax; and in the tenth, a piece of barley-cake cakes.\n",
      "\n",
      "And in the tenth piece of white bread cakes ye shall have a piece of white bread cakes; and in the eleventh a piece of flax; and in the twelfth, a piece of barley-cake cakes.\n",
      "\n",
      "And in the twelfth piece of white bread cakes ye shall have a piece of white bread cakes; and in the thirteenth, a piece of white bread cakes.\n",
      "\n",
      "And in the thirteenth piece of white bread cakes ye shall have a piece of white bread cakes; and in the fourteenth, a piece of white bread cakes; and in the fifteenth, one piece of flax; and in the sixteenth, a piece of barley-cake cakes.\n",
      "\n",
      "And in the seventh piece of flax ye shall have a piece of flax; and in the eighth piece of flax ye shall have a piece of flax; and in the ninth, a piece of flax; and in the tenth, a piece\n",
      "---------------\n",
      "\n",
      "\n",
      "From the U.S. standpoint, the idea of a U.S. nuclear weapons program over and above its nuclear shield and its nuclear missiles program would be a shocking development.\n",
      "\n",
      "A President Clinton or a President Clinton-appointee might very well have to decide whether to use nuclear weapons. Both might use them. The second president might not.\n",
      "\n",
      "But both might or might not have to decide whether to use it.\n",
      "\n",
      "And to that question President Ford must perhaps be admitted: 'If the United States does not go to war with Japan, I believe it is the best way to keep the peace. If the United States does not go to war with Japan, it is the worst way.'\n",
      "\n",
      "And to that question President Truman must probably be admitted: 'If the United States does not go to war with Japan, I do not believe it is the worst way to keep the peace.'\n",
      "\n",
      "I think this is a very important question, and I think it has a very important answer.\n",
      "\n",
      "I take it that you will accept the answer that you have given, and you may hold your peace, and you may have peace.\n",
      "\n",
      "If the North American free states (concerning which I would not speak, for I am not the President of the United States) were to decide to use U.S. nuclear weapons, as a matter of principle, against the Japanese nation, then U.S. policy would be directed, as it was directed by the President, in one direction; in the other direction, the protection of the world.\n",
      "Over the years I have asked you to speak and direct our policy in relation to the North American free states.\n",
      "But you have refused.\n",
      "\n",
      "I am not asking because I do not want to have your peace; I am seeking your peace. You have refused to do it.\n",
      "\n",
      "I am asking because I am angry at how you have refused to do my duty.\n",
      "\n",
      "Well, I have refused to do the duty you have refused to do because I am not the President of the United States. I have refused to do the duty because I have not the benefit of your wisdom. You have had to do the duty.\n",
      "\n",
      "I have had to do the duty because I have not the benefit of your wisdom.\n",
      "How can I ever trust you, if you have refused to do my duty? How can I,\n",
      "being so many miles away, trust you not to do\n",
      "---------------\n",
      "\n",
      "\n",
      "A second time, she did it again.\n",
      "\n",
      "\"By Allah! Thou didst not do it if thou didst not know that I was, and that I knew not thou: thou didst do it if thou hadst not had time to learn that I did then.\"\n",
      "Henceforth, he said, she was all her own, and she pleased him, so that he spoke to her.\n",
      "\n",
      "'Now,' says a girl, 'if thou didst not know that, then I did; and that thou didst not know that, then I did.'\n",
      "\n",
      "'So then, my wife,' said the boy, 'I know what I do and what thou doest.\n",
      "\n",
      "'I'm your brother, not your master, boy!'\n",
      "\n",
      "'I'm your nephew, not your master, boy!'\n",
      "\n",
      "'I'm your uncle, not your master, boy!'\n",
      "\n",
      "'I'm your cousin, not your master, boy!'\n",
      "\n",
      "'I'm your sister, not your master, boy!'\n",
      "\n",
      "'I'm your sister, not your master, boy!'\n",
      "\n",
      "'I'm your cousin, not your master, boy!'\n",
      "\n",
      "'I'm your sister, not your master, boy!'\n",
      "\n",
      "'I'm your uncle, not your master, boy!'\n",
      "\n",
      "'I'm your aunt, not your master, boy!'\n",
      "\n",
      "'I'm your aunt, not your master, boy!'\n",
      "\n",
      "'I'm thy uncle, not thy master, boy!'\n",
      "\n",
      "'I'm thy aunt, not thy master, boy!'\n",
      "\n",
      "'I'm thy cousin, not thy master, boy!'\n",
      "\n",
      "'I'm thy aunt, not thy master, boy!'\n",
      "\n",
      "'I'm my niece, not my master, boy!'\n",
      "\n",
      "'I'm thy cousin, not thy master, boy!'\n",
      "\n",
      "'I'm thy aunt, not thy master, boy!'\n",
      "\n",
      "'I'm thy niece, not thy master, boy!'\n",
      "\n",
      "'I'm thy aunt, not thy master, boy!'\n",
      "\n",
      "'I'm thy uncle, not thy master, boy!'\n",
      "\n",
      "'I'm thy cousin, not thy master, boy!'\n",
      "\n",
      "'I'm thy aunt, not thy master, boy!'\n",
      "\n",
      "'I'm thy aunt, not thy master, boy!'\n",
      "\n",
      "'I'm thy niece, not thy master, boy!'\n",
      "\n",
      "'I'm thy\n",
      "---------------\n",
      "\n",
      "\n",
      "The second is a strange and strange work; for it is very false, being that the same way is applied to all this; but as the God of the world hath set, and laid it in the earth, it is no more abominable than to have it to dispose of.\n",
      "\n",
      "The third is that we should do the like.\n",
      "\n",
      "What, indeed, is the good of it?\n",
      "\n",
      "If we do the contrary, we do both:\n",
      "We do the good of evil, and do the evil of good.\n",
      "\n",
      "What, then, is the evil of it?\n",
      "\n",
      "That which is contrary to good and to evil.\n",
      "\n",
      "Now, then, what is the good of it?\n",
      "\n",
      "To be free.\n",
      "\n",
      "What is the evil of it?\n",
      "\n",
      "That we do the same thing, and do that too.\n",
      "\n",
      "What is the good of it?\n",
      "O, have you a mind for this? What is the evil of it, if we should do the same thing as is contrary to good, and do the contrary too?\n",
      "\n",
      "First, that we do the same thing:\n",
      "Now, then, what is the evil of it?\n",
      "\n",
      "That we do the same thing as is contrary to good, and do the contrary too.\n",
      "\n",
      "What is the good of it?\n",
      "\n",
      "O, have you not a mind for this? what is the evil of it, if we should do the same thing as is contrary to good, and do the contrary too?\n",
      "\n",
      "First, that we do the same thing:\n",
      "Now, then, what is the evil of it?\n",
      "\n",
      "That we do the same thing as is contrary to good, and do the contrary too.\n",
      "\n",
      "What is the good of it?\n",
      "\n",
      "O, have you not a mind for this? what is the evil of it, if we should do the same thing as is contrary to good, and do the contrary too?\n",
      "\n",
      "First, that we do the same thing:\n",
      "Now, then, what is the evil of it?\n",
      "\n",
      "That we do the same thing as is contrary to good, and do the contrary too.\n",
      "\n",
      "What is the good of it?\n",
      "\n",
      "O, have you not a mind for this? what is the evil of it, if we should do the same thing as is contrary to good, and do the contrary too?\n",
      "\n",
      "First, that we do the same\n",
      "---------------\n",
      "\n",
      "THE PROPHETS ARE PROMISING...THE ARCHIES ARE PROPHETS....THE TYRANIDS ARE PROMISING...THE EUROPHETS ARE PROMISING...THE NATIONS ARE PROMISING...THE KINGDOMS ARE PROMISING...THE KINGDOMS ARE PROMISING...\n",
      "\n",
      "\n",
      "As the threefold world-dominion grows, so are the powers of the nations. The world-wrath of the arch-ruler shall be made light; and when he be dead, the arch-ruler shall be buried.\n",
      "\n",
      "Revelation 11:1-23\n",
      "\n",
      "Here is Peter, a king's son, a mighty man, anointed with the Holy Spirit; and the sons of men shall love him as their father, and shall worship him as their God.\n",
      "We believe in one God, the Father Almighty, Maker of heaven and earth, of all that is in them, of all that is in the firmament of the heaven above, and of all things that are in the earth beneath.\n",
      "\n",
      "And to him be glory forever and ever. Amen.\n",
      "But now I tell you, what is the use of your bodies, but to serve? But now I tell you, that the body is the instrument of the glory, and the glory is the service of the body.\n",
      "\n",
      "Proverbs 11:23-28\n",
      "\n",
      "Now these are the words of the Lord:\n",
      "The earth shall turn,\n",
      "And the fountains of the deep shall be filled with his foot:\n",
      "The waters shall run, and the things in the deep shall hide.\n",
      "\n",
      "Proverbs 11:29\n",
      "\n",
      "Then shall ye lie down in the dust:\n",
      "And the air shall be darkness, and the spirit of darkness.\n",
      "\n",
      "Proverbs 11:30\n",
      "\n",
      "My soul, that beareth witness of me, and my spirit, that bear witness of me,\n",
      "All live for glory in my name: for a man's work is a noble work;\n",
      "For a man's world a noble world:\n",
      "And to a worldly master is a pleasing master:\n",
      "But for a king, who is the least of all,\n",
      "He makes his house a house of pain:\n",
      "And when he hath made joy his work, he shall make pain his repast.\n",
      "\n",
      "Proverbs 11:31\n",
      "\n",
      "My soul, that beareth witness of me,\n",
      "\n",
      "---------------\n",
      "\n",
      "\n",
      "Merely that the subject of my counsel was not worth the words which I now spoke:\n",
      "'But me, methinks, the time hath come: I know not the hour.\n",
      "\n",
      "'Now, methinks,' quoth he, 'that thou shalt be a debtor.\n",
      "\n",
      "'I say to thee, thou shouldst be one of the lenders.\n",
      "\n",
      "'Now, methinks,' quoth he, 'that thou shalt be a debtor for ever.\n",
      "\n",
      "'Now,' quoth he, 'that thou shalt be a debtor for ever.\n",
      "\n",
      "'I say to thee, thou shouldst be one of the lenders.\n",
      "I say to thee, thou shouldst be one of the lenders.\n",
      "\n",
      "'I say to thee,' quoth he, 'that thou shouldst be a debtor for ever.\n",
      "\n",
      "I say unto thee, thou shouldst be one of the lenders.\n",
      "\n",
      "'I say to thee, thou shouldst be one of the lenders.\n",
      "\n",
      "'I say to thee,' quoth he, 'that thou shouldst be a debtor for ever.\n",
      "The time hath come, methinks, that I shall be one of the lenders.\n",
      "\n",
      "'The time hath come, methinks, that I shall be one of the lenders.\n",
      "\n",
      "'The time hath come, methinks, that I shall be one of the lenders.\n",
      "\n",
      "'The time hath come, methinks, that I shall be one of the lenders.\n",
      "\n",
      "'The time hath come, methinks, that I shall be one of the lenders.\n",
      "\n",
      "'The time hath come, methinks, that I shall be one of the lenders.\n",
      "\n",
      "'The time hath come, methinks, that I shall be one of the lenders,\n",
      "'And therefore go all to and fro in a vein and a barrow,\n",
      "Till I see my master and my brother and my friends\n",
      "What debt I shall owe to him.\n",
      "\n",
      "'And therefore go all to and fro in a vein and a barrow,\n",
      "Till I see my master and my brother and my friends,\n",
      "Is a lender, a debtor, a debtor, a debtor, a debtor, a debtor, a debtor, a debtor, a debtor, a debtor, a debtor.\n",
      "\n",
      "'The time has come, methinks, that I shall be a lender.\n",
      "Some of my kindred have I so long years\n",
      "---------------\n",
      "\n",
      "\n",
      "The first attempt was a little rough, and\n",
      "The boy was well made a coward.\n",
      "Shrewsbury:\n",
      "Nay, he was a good boy; but he\n",
      "Is mad to drink, and he knows it\n",
      "So well knows not his heart, that when\n",
      "He was served with the bottle wept,\n",
      "He gave the man so much money\n",
      "He did be mad to drink of it: we\n",
      "Will look how he did use it.\n",
      "\n",
      "Hatch:\n",
      "Come, Sir John, and welcome hither.\n",
      "\n",
      "Shrewsbury:\n",
      "Dearest Hatch, I give my greetings\n",
      "To Sir John Cholmonde; and I know\n",
      "To-morrow, and to-morrow, with more\n",
      "To-morrow, will be mine.\n",
      "Thence with my Lord Archer, to the Tower\n",
      "And there we met with my Lord's father,\n",
      "To-day: and the next day, with my Lord\n",
      "Deacon, he came to meet me,\n",
      "And we did talk of the discourse\n",
      "He had with me of a noble wife,\n",
      "Which he did make me know was a\n",
      "lie, with a boy, who had asked to\n",
      "meet him at court, for the benefit of\n",
      "the worthy King.\n",
      "\n",
      "Thence with my Lord, to the Tower, and there we\n",
      "met with my Lord's father, 'twixt\n",
      "clumsiness and chagrin.\n",
      "After dinner, and mighty well with my Lord,\n",
      "I to the office, and there sat all the morning\n",
      "Doing with my Lord. At noon, Mr. Robert Pepys,\n",
      "Mr. Wren, and Mr. Wren's brother, by appointment,\n",
      "All the afternoon in the garden with the boys\n",
      "to till the garden: and then, to the\n",
      "night, in the garden with the boys, I\n",
      "went in company with my Lord and others\n",
      "to the Lark; where I had many good company\n",
      "For the purpose of the exercise of an\n",
      "ordinance, in all that I would say.\n",
      "What is the matter, sir?\n",
      "Come, I'll tell you myself of it.\n",
      "Wren:\n",
      "What business is that?\n",
      "\n",
      "Robert:\n",
      "The discourse with which the Duke of York\n",
      "Went to Chancery, in the person of Sir\n",
      "Gentlemen Gilbert and Gilbert.\n",
      "\n",
      "Wren:\n",
      "What do you mean?\n",
      "\n",
      "Robert:\n",
      "---------------\n",
      "\n",
      "SINCE the next Congress shall be convened, the Lord hath imposed upon the people, as I have commanded him, that they shall not exercise their rights, unless they shall present a voice against all bills. And therefore the people have taken up a petition of their grievances, and have availed themselves of the means of redress; that the Commons, therefore, if they shall in any wise please to take up their grievances against any one, may deliver their bill.\n",
      "\n",
      "SICINIUS:\n",
      "Whom shall it please you to deliver? The majority? Nor must they deliver it; but must it be delivered in their bill.\n",
      "\n",
      "SINUS:\n",
      "Then shall they deliver their bill, and they shall have a voice against it;\n",
      "Or they shall deliver their bill, and they shall not a voice against it;\n",
      "But, when they shall have a voice against it, must you not in any wise deliver that bill.\n",
      "\n",
      "SICINIUS:\n",
      "How will you deliver it, then?\n",
      "\n",
      "SINUS:\n",
      "I shall deliver it in my bill: and therefore, unless you deliver your bill,\n",
      "You will not be a voice against it.\n",
      "\n",
      "SICINIUS:\n",
      "No, Sir; unless you deliver your bill.\n",
      "\n",
      "SINUS:\n",
      "But the Bill is yours;\n",
      "And therefore, unless you deliver your bill, you shall not be a voice against it;\n",
      "Unless you deliver your bill, you may not be a voice against it.\n",
      "\n",
      "SICINIUS:\n",
      "What then?\n",
      "\n",
      "SINUS:\n",
      "I will not deliver my bill; but I will deliver it in some other way:\n",
      "And therefore, if you deliver your bill, I will deliver your bill in some other way,\n",
      "And in my bill, not in your bill.\n",
      "\n",
      "SICINIUS:\n",
      "But you will not deliver your bill; it is your own\n",
      "You will not deliver your bill; it is your own.\n",
      "\n",
      "SINUS:\n",
      "I do not believe you.\n",
      "\n",
      "SICINIUS:\n",
      "I do not believe you.\n",
      "\n",
      "SICINIUS:\n",
      "Sir, how will you deliver your bill?\n",
      "\n",
      "SINUS:\n",
      "I will deliver it in the bill I mentioned; when I shall have a voice against it.\n",
      "\n",
      "SICINIUS:\n",
      "Whom shall it please, then, to\n",
      "---------------\n",
      "\n",
      "As the ball touches the ground, it has a speed of over thirty miles an hour.\n",
      "\n",
      "A different instrument is now used by him.\n",
      "\n",
      "When you were born, you were a peasant of Bohemia.\n",
      "\n",
      "Now you are a nobleman of Venice.\n",
      "\n",
      "The old man is a rascal.\n",
      "\n",
      "Do you not remember us when you were so little?\n",
      "\n",
      "I did.\n",
      "\n",
      "We were always in the service of the monarchy.\n",
      "In the beginning, our master was a stranger.\n",
      "My father was a prince of Bohemia.\n",
      "\n",
      "You did not know him?\n",
      "\n",
      "I did not.\n",
      "\n",
      "What made you choose him?\n",
      "\n",
      "I did not.\n",
      "\n",
      "What was his name?\n",
      "\n",
      "The monarch of France.\n",
      "\n",
      "Why?\n",
      "\n",
      "He was the son of the king.\n",
      "\n",
      "When did he become king?\n",
      "\n",
      "Late in the year of our Lord twelve hundred and eighty-one.\n",
      "We were all in service.\n",
      "I was a boy of twelve years old.\n",
      "\n",
      "How would you describe him?\n",
      "\n",
      "A child of twelve years old.\n",
      "One of his eyes was red with blood.\n",
      "\n",
      "My father would have no greater son.\n",
      "\n",
      "Are you a nobleman?\n",
      "\n",
      "My father has been very well.\n",
      "\n",
      "What then?\n",
      "\n",
      "He has a son.\n",
      "\n",
      "What then?\n",
      "\n",
      "He has a daughter.\n",
      "\n",
      "What then?\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "\n",
      "She has an acquaintance with my father.\n",
      "\n",
      "My father would have no greater son.\n",
      "\n",
      "What then?\n",
      "\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "\n",
      "He has an acquaintance with my mother.\n",
      "\n",
      "My son is his right.\n",
      "\n",
      "My son is married.\n",
      "\n",
      "What then?\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "\n",
      "He is married.\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "\n",
      "He is married.\n",
      "\n",
      "What then?\n",
      "\n",
      "He is married.\n",
      "\n",
      "In what country did you live?\n",
      "\n",
      "Our country is not far from here.\n",
      "\n",
      "My country is near France.\n",
      "\n",
      "My son is his right.\n",
      "\n",
      "His wife?\n",
      "\n",
      "Master to my father.\n",
      "\n",
      "Master to my daughter.\n",
      "\n",
      "Why should my daughter choose to marry a\n",
      "---------------\n",
      "\n",
      "He'd been under the impression that neither God nor man were but souls, that therefore in him had gone forth to possess nature.\n",
      "\n",
      "The Lord's person is like a lion, which goes on continually to chew, and eats nothing but what is there left:\n",
      "His flesh is like an eel, which chews when it should chew.\n",
      "\n",
      "He stood weeping, and the Lord said:\n",
      "Behold, I give thee peace, thou and thy wife, and thy son;\n",
      "Which thou hast not loved, neither have I;\n",
      "Nor have I loved thee, for I have loved another.\n",
      "\n",
      "LADY CORDELIA:\n",
      "Enlist me, and I will keep thy peace;\n",
      "And he, which thou hast loved, and thy son,\n",
      "Which thou hast not loved, neither have I.\n",
      "\n",
      "THE LORD:\n",
      "Come, let us in.\n",
      "\n",
      "LADY CORDELIA:\n",
      "I will seek the queen of the Britons,\n",
      "And deliver her into thy hands,\n",
      "Which he hath so far so easily found;\n",
      "And from her I will bring thee and thy son,\n",
      "Whom I knew not what I have found.\n",
      "\n",
      "THE LORD:\n",
      "What have we found?\n",
      "\n",
      "LADY CORDELIA:\n",
      "The body of a lion, who, though he was but a lion,\n",
      "Was as the lion had gone to eat the flesh of a woman.\n",
      "\n",
      "THE LORD:\n",
      "What shall we do with him?\n",
      "\n",
      "LADY CORDELIA:\n",
      "To seek vengeance.\n",
      "\n",
      "THE LORD:\n",
      "Who shall he be?\n",
      "\n",
      "LADY CORDELIA:\n",
      "O, I'll be true, and, O, my joy,\n",
      "As he is true and as I am, he shall be true and true;\n",
      "I shall be true and true, that will find and find him.\n",
      "\n",
      "THE LORD:\n",
      "Let us go, let us go.\n",
      "\n",
      "THE LORD:\n",
      "Ay, and we'll find him; he shall be found.\n",
      "\n",
      "LADY CORDELIA:\n",
      "Away, away, thy husband; for I'll be true.\n",
      "But, lord, where is the king?\n",
      "\n",
      "THE LORD:\n",
      "Here.\n",
      "\n",
      "LADY CORDELIA:\n",
      "Here, let's go: where is the king?\n",
      "With whom shall we meet?\n",
      "She is still alive\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb894955-7f07-4025-bc18-d78021cf1603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
